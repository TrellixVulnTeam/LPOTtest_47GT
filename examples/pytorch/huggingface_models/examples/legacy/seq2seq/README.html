

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Sequence-to-Sequence Training and Evaluation &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/index.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Sequence-to-Sequence Training and Evaluation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../../_sources/examples/pytorch/huggingface_models/examples/legacy/seq2seq/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="sequence-to-sequence-training-and-evaluation">
<h1>Sequence-to-Sequence Training and Evaluation<a class="headerlink" href="#sequence-to-sequence-training-and-evaluation" title="Permalink to this headline">¶</a></h1>
<p>This directory contains examples for finetuning and evaluating transformers on summarization and translation tasks.
For deprecated <code class="docutils literal notranslate"><span class="pre">bertabs</span></code> instructions, see <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/research_projects/bertabs/README.md"><code class="docutils literal notranslate"><span class="pre">bertabs/README.md</span></code></a>.</p>
<div class="section" id="supported-architectures">
<h2>Supported Architectures<a class="headerlink" href="#supported-architectures" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MarianMTModel</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PegasusForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MBartForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FSMTForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></p></li>
</ul>
</div>
<div class="section" id="downlowd-the-datasets">
<h2>Downlowd the Datasets<a class="headerlink" href="#downlowd-the-datasets" title="Permalink to this headline">¶</a></h2>
<div class="section" id="xsum">
<h3>XSUM<a class="headerlink" href="#xsum" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/legacy/seq2seq
wget https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz
tar -xzvf xsum.tar.gz
<span class="nb">export</span> <span class="nv">XSUM_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/xsum
</pre></div>
</div>
<p>this should make a directory called <code class="docutils literal notranslate"><span class="pre">xsum/</span></code> with files like <code class="docutils literal notranslate"><span class="pre">test.source</span></code>.
To use your own data, copy that files format. Each article to be summarized is on its own line.</p>
</div>
<div class="section" id="cnn-dailymail">
<h3>CNN/DailyMail<a class="headerlink" href="#cnn-dailymail" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/legacy/seq2seq
wget https://cdn-datasets.huggingface.co/summarization/cnn_dm_v2.tgz
tar -xzvf cnn_dm_v2.tgz  <span class="c1"># empty lines removed</span>
mv cnn_cln cnn_dm
<span class="nb">export</span> <span class="nv">CNN_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/cnn_dm
</pre></div>
</div>
<p>this should make a directory called <code class="docutils literal notranslate"><span class="pre">cnn_dm/</span></code> with 6 files.</p>
</div>
<div class="section" id="wmt16-english-romanian-translation-data">
<h3>WMT16 English-Romanian Translation Data<a class="headerlink" href="#wmt16-english-romanian-translation-data" title="Permalink to this headline">¶</a></h3>
<p>download with this command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://cdn-datasets.huggingface.co/translation/wmt_en_ro.tar.gz
tar -xzvf wmt_en_ro.tar.gz
<span class="nb">export</span> <span class="nv">ENRO_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/wmt_en_ro
</pre></div>
</div>
<p>this should make a directory called <code class="docutils literal notranslate"><span class="pre">wmt_en_ro/</span></code> with 6 files.</p>
</div>
<div class="section" id="wmt-english-german">
<h3>WMT English-German<a class="headerlink" href="#wmt-english-german" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://cdn-datasets.huggingface.co/translation/wmt_en_de.tgz
tar -xzvf wmt_en_de.tgz
<span class="nb">export</span> <span class="nv">DATA_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/wmt_en_de
</pre></div>
</div>
</div>
<div class="section" id="fsmt-datasets-wmt">
<h3>FSMT datasets (wmt)<a class="headerlink" href="#fsmt-datasets-wmt" title="Permalink to this headline">¶</a></h3>
<p>Refer to the scripts starting with <code class="docutils literal notranslate"><span class="pre">eval_</span></code> under:
https://github.com/huggingface/transformers/tree/master/scripts/fsmt</p>
</div>
<div class="section" id="pegasus-multiple-datasets">
<h3>Pegasus (multiple datasets)<a class="headerlink" href="#pegasus-multiple-datasets" title="Permalink to this headline">¶</a></h3>
<p>Multiple eval datasets are available for download from:
https://github.com/stas00/porting/tree/master/datasets/pegasus</p>
</div>
<div class="section" id="your-data">
<h3>Your Data<a class="headerlink" href="#your-data" title="Permalink to this headline">¶</a></h3>
<p>If you are using your own data, it must be formatted as one directory with 6 files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">source</span>
<span class="n">train</span><span class="o">.</span><span class="n">target</span>
<span class="n">val</span><span class="o">.</span><span class="n">source</span>
<span class="n">val</span><span class="o">.</span><span class="n">target</span>
<span class="n">test</span><span class="o">.</span><span class="n">source</span>
<span class="n">test</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.source</span></code> files are the input, the <code class="docutils literal notranslate"><span class="pre">.target</span></code> files are the desired output.</p>
</div>
</div>
<div class="section" id="potential-issues">
<h2>Potential issues<a class="headerlink" href="#potential-issues" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>native AMP (<code class="docutils literal notranslate"><span class="pre">--fp16</span></code> and no apex) may lead to a huge memory leak and require 10x gpu memory. This has been fixed in pytorch-nightly and the minimal official version to have this fix will be pytorch-1.7.1. Until then if you have to use mixed precision please use AMP only with pytorch-nightly or NVIDIA’s apex. Reference: https://github.com/huggingface/transformers/issues/8403</p></li>
</ul>
</div>
<div class="section" id="tips-and-tricks">
<h2>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Permalink to this headline">¶</a></h2>
<p>General Tips:</p>
<ul>
<li><p>since you need to run from <code class="docutils literal notranslate"><span class="pre">examples/legacy/seq2seq</span></code>, and likely need to modify code, the easiest workflow is fork transformers, clone your fork, and run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-e</span> <span class="pre">.</span></code> before you get started.</p></li>
<li><p>try <code class="docutils literal notranslate"><span class="pre">--freeze_encoder</span></code> or <code class="docutils literal notranslate"><span class="pre">--freeze_embeds</span></code> for faster training/larger batch size.  (3hr per epoch with bs=8, see the “xsum_shared_task” command below)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fp16_opt_level=O1</span></code> (the default works best).</p></li>
<li><p>In addition to the pytorch-lightning .ckpt checkpoint, a transformers checkpoint will be saved.
Load it with <code class="docutils literal notranslate"><span class="pre">BartForConditionalGeneration.from_pretrained(f'{output_dir}/best_tfmr)</span></code>.</p></li>
<li><p>At the moment, <code class="docutils literal notranslate"><span class="pre">--do_predict</span></code> does not work in a multi-gpu setting. You need to use <code class="docutils literal notranslate"><span class="pre">evaluate_checkpoint</span></code> or the <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code> code.</p></li>
<li><p>This warning can be safely ignored:</p>
<blockquote>
<div><p>“Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-xsum and are newly initialized: [‘final_logits_bias’]”</p>
</div></blockquote>
</li>
<li><p>Both finetuning and eval are 30% faster with <code class="docutils literal notranslate"><span class="pre">--fp16</span></code>. For that you need to <a class="reference external" href="https://github.com/NVIDIA/apex#quick-start">install apex</a>.</p></li>
<li><p>Read scripts before you run them!</p></li>
</ul>
<p>Summarization Tips:</p>
<ul class="simple">
<li><p>(summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB GPU RAM with fp16 on an NVIDIA-V100.</p></li>
<li><p>If you want to run experiments on improving the summarization finetuning process, try the XSUM Shared Task (below). It’s faster to train than CNNDM because the summaries are shorter.</p></li>
<li><p>For CNN/DailyMail, the default <code class="docutils literal notranslate"><span class="pre">val_max_target_length</span></code> and <code class="docutils literal notranslate"><span class="pre">test_max_target_length</span></code> will truncate the ground truth labels, resulting in slightly higher rouge scores. To get accurate rouge scores, you should rerun calculate_rouge on the <code class="docutils literal notranslate"><span class="pre">{output_dir}/test_generations.txt</span></code> file saved by <code class="docutils literal notranslate"><span class="pre">trainer.test()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max_target_length=60</span> <span class="pre">--val_max_target_length=60</span> <span class="pre">--test_max_target_length=100</span> </code> is a reasonable setting for XSUM.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wandb</span></code> can be used by specifying <code class="docutils literal notranslate"><span class="pre">--logger_name</span> <span class="pre">wandb</span></code>. It is useful for reproducibility. Specify the environment variable <code class="docutils literal notranslate"><span class="pre">WANDB_PROJECT='hf_xsum'</span></code> to do the XSUM shared task.</p></li>
<li><p>If you are finetuning on your own dataset, start from <code class="docutils literal notranslate"><span class="pre">distilbart-cnn-12-6</span></code> if you want long summaries and <code class="docutils literal notranslate"><span class="pre">distilbart-xsum-12-6</span></code> if you want short summaries.
(It rarely makes sense to start from <code class="docutils literal notranslate"><span class="pre">bart-large</span></code> unless you are a researching finetuning methods).</p></li>
</ul>
<p><strong>Update 2018-07-18</strong>
Datasets: <code class="docutils literal notranslate"><span class="pre">LegacySeq2SeqDataset</span></code> will be used for all tokenizers without a <code class="docutils literal notranslate"><span class="pre">prepare_seq2seq_batch</span></code> method. Otherwise, <code class="docutils literal notranslate"><span class="pre">Seq2SeqDataset</span></code> will be used.
Future work/help wanted: A new dataset to support multilingual tasks.</p>
</div>
<div class="section" id="fine-tuning-using-seq2seqtrainer">
<h2>Fine-tuning using Seq2SeqTrainer<a class="headerlink" href="#fine-tuning-using-seq2seqtrainer" title="Permalink to this headline">¶</a></h2>
<p>To use <code class="docutils literal notranslate"><span class="pre">Seq2SeqTrainer</span></code> for fine-tuning you should use the <code class="docutils literal notranslate"><span class="pre">finetune_trainer.py</span></code> script. It subclasses <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> to extend it for seq2seq training. Except the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>-related <code class="docutils literal notranslate"><span class="pre">TrainingArguments</span></code>, it shares the same argument names as that of <code class="docutils literal notranslate"><span class="pre">finetune.py</span></code> file. One notable difference is that calculating generative metrics (BLEU, ROUGE) is optional and is controlled using the <code class="docutils literal notranslate"><span class="pre">--predict_with_generate</span></code> argument.</p>
<p>With PyTorch 1.6+ it’ll automatically use <code class="docutils literal notranslate"><span class="pre">native</span> <span class="pre">AMP</span></code> when <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> is set.</p>
<p>To see all the possible command line options, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python finetune_trainer.py --help
</pre></div>
</div>
<p>For multi-gpu training use <code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code>, e.g. with 2 gpus:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">2</span>  finetune_trainer.py ...
</pre></div>
</div>
<p><strong>At the moment, <code class="docutils literal notranslate"><span class="pre">Seq2SeqTrainer</span></code> does not support <em>with teacher</em> distillation.</strong></p>
<p>All <code class="docutils literal notranslate"><span class="pre">Seq2SeqTrainer</span></code>-based fine-tuning scripts are included in the <code class="docutils literal notranslate"><span class="pre">builtin_trainer</span></code> directory.</p>
<div class="section" id="tpu-training">
<h3>TPU Training<a class="headerlink" href="#tpu-training" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Seq2SeqTrainer</span></code> supports TPU training with few caveats</p>
<ol class="simple">
<li><p>As <code class="docutils literal notranslate"><span class="pre">generate</span></code> method does not work on TPU at the moment, <code class="docutils literal notranslate"><span class="pre">predict_with_generate</span></code> cannot be used. You should use <code class="docutils literal notranslate"><span class="pre">--prediction_loss_only</span></code> to only calculate loss, and do not set <code class="docutils literal notranslate"><span class="pre">--do_predict</span></code> and <code class="docutils literal notranslate"><span class="pre">--predict_with_generate</span></code>.</p></li>
<li><p>All sequences should be padded to be of equal length to avoid extremely slow training. (<code class="docutils literal notranslate"><span class="pre">finetune_trainer.py</span></code> does this automatically when running on TPU.)</p></li>
</ol>
<p>We provide a very simple launcher script named <code class="docutils literal notranslate"><span class="pre">xla_spawn.py</span></code> that lets you run our example scripts on multiple TPU cores without any boilerplate. Just pass a <code class="docutils literal notranslate"><span class="pre">--num_cores</span></code> flag to this script, then your regular training script with its arguments (this is similar to the <code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> helper for <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>).</p>
<p><code class="docutils literal notranslate"><span class="pre">builtin_trainer/finetune_tpu.sh</span></code> script provides minimal arguments needed for TPU training.</p>
<p>The following command fine-tunes <code class="docutils literal notranslate"><span class="pre">sshleifer/student_marian_en_ro_6_3</span></code> on TPU V3-8 and should complete one epoch in ~5-6 mins.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./builtin_trainer/train_distil_marian_enro_tpu.sh
</pre></div>
</div>
</div>
</div>
<div class="section" id="evaluation-commands">
<h2>Evaluation Commands<a class="headerlink" href="#evaluation-commands" title="Permalink to this headline">¶</a></h2>
<p>To create summaries for each article in dataset, we use <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code>, here are a few commands that run eval for different tasks and models.
If ‘translation’ is in your task name, the computed metric will be BLEU. Otherwise, ROUGE will be used.</p>
<p>For t5, you need to specify –task translation_{src}<em>to</em>{tgt} as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">DATA_DIR</span><span class="o">=</span>wmt_en_ro
./run_eval.py t5-base <span class="se">\</span>
    <span class="nv">$DATA_DIR</span>/val.source t5_val_generations.txt <span class="se">\</span>
    --reference_path <span class="nv">$DATA_DIR</span>/val.target <span class="se">\</span>
    --score_path enro_bleu.json <span class="se">\</span>
    --task translation_en_to_ro <span class="se">\</span>
    --n_obs <span class="m">100</span> <span class="se">\</span>
    --device cuda <span class="se">\</span>
    --fp16 <span class="se">\</span>
    --bs <span class="m">32</span>
</pre></div>
</div>
<p>This command works for MBART, although the BLEU score is suspiciously low.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">DATA_DIR</span><span class="o">=</span>wmt_en_ro
./run_eval.py facebook/mbart-large-en-ro <span class="nv">$DATA_DIR</span>/val.source mbart_val_generations.txt <span class="se">\</span>
    --reference_path <span class="nv">$DATA_DIR</span>/val.target <span class="se">\</span>
    --score_path enro_bleu.json <span class="se">\</span>
    --task translation <span class="se">\</span>
    --n_obs <span class="m">100</span> <span class="se">\</span>
    --device cuda <span class="se">\</span>
    --fp16 <span class="se">\</span>
    --bs <span class="m">32</span>
</pre></div>
</div>
<p>Summarization (xsum will be very similar):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">DATA_DIR</span><span class="o">=</span>cnn_dm
./run_eval.py sshleifer/distilbart-cnn-12-6 <span class="nv">$DATA_DIR</span>/val.source dbart_val_generations.txt <span class="se">\</span>
    --reference_path <span class="nv">$DATA_DIR</span>/val.target <span class="se">\</span>
    --score_path cnn_rouge.json <span class="se">\</span>
    --task summarization <span class="se">\</span>
    --n_obs <span class="m">100</span> <span class="se">\</span>

th <span class="m">56</span> <span class="se">\</span>
    --fp16 <span class="se">\</span>
    --bs <span class="m">32</span>
</pre></div>
</div>
<div class="section" id="multi-gpu-evaluation">
<h3>Multi-GPU Evaluation<a class="headerlink" href="#multi-gpu-evaluation" title="Permalink to this headline">¶</a></h3>
<p>here is a command to run xsum evaluation on 8 GPUS. It is more than linearly faster than run_eval.py in some cases
because it uses SortishSampler to minimize padding. You can also use it on 1 GPU. <code class="docutils literal notranslate"><span class="pre">data_dir</span></code> must have
<code class="docutils literal notranslate"><span class="pre">{type_path}.source</span></code> and <code class="docutils literal notranslate"><span class="pre">{type_path}.target</span></code>. Run <code class="docutils literal notranslate"><span class="pre">./run_distributed_eval.py</span> <span class="pre">--help</span></code> for all clargs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">8</span>  run_distributed_eval.py <span class="se">\</span>
    --model_name sshleifer/distilbart-large-xsum-12-3  <span class="se">\</span>
    --save_dir xsum_generations <span class="se">\</span>
    --data_dir xsum <span class="se">\</span>
    --fp16  <span class="c1"># you can pass generate kwargs like num_beams here, just like run_eval.py</span>
</pre></div>
</div>
<p>Contributions that implement this command for other distributed hardware setups are welcome!</p>
<div class="section" id="single-gpu-eval-tips-and-tricks">
<h4>Single-GPU Eval: Tips and Tricks<a class="headerlink" href="#single-gpu-eval-tips-and-tricks" title="Permalink to this headline">¶</a></h4>
<p>When using <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code>, the following features can be useful:</p>
<ul>
<li><p>if you running the script multiple times and want to make it easier to track what arguments produced that output, use <code class="docutils literal notranslate"><span class="pre">--dump-args</span></code>. Along with the results it will also dump any custom params that were passed to the script. For example if you used: <code class="docutils literal notranslate"><span class="pre">--num_beams</span> <span class="pre">8</span> <span class="pre">--early_stopping</span> <span class="pre">true</span></code>, the output will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;bleu&#39;</span><span class="p">:</span> <span class="mf">26.887</span><span class="p">,</span> <span class="s1">&#39;n_obs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;runtime&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;seconds_per_sample&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;num_beams&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;early_stopping&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--info</span></code> is an additional argument available for the same purpose of tracking the conditions of the experiment. It’s useful to pass things that weren’t in the argument list, e.g. a language pair <code class="docutils literal notranslate"><span class="pre">--info</span> <span class="pre">&quot;lang:en-ru&quot;</span></code>. But also if you pass <code class="docutils literal notranslate"><span class="pre">--info</span></code> without a value it will fallback to the current date/time string, e.g. <code class="docutils literal notranslate"><span class="pre">2020-09-13</span> <span class="pre">18:44:43</span></code>.</p>
<p>If using <code class="docutils literal notranslate"><span class="pre">--dump-args</span> <span class="pre">--info</span></code>, the output will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;bleu&#39;</span><span class="p">:</span> <span class="mf">26.887</span><span class="p">,</span> <span class="s1">&#39;n_obs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;runtime&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;seconds_per_sample&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;num_beams&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;early_stopping&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;info&#39;</span><span class="p">:</span> <span class="s1">&#39;2020-09-13 18:44:43&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>If using <code class="docutils literal notranslate"><span class="pre">--dump-args</span> <span class="pre">--info</span> <span class="pre">&quot;pair:en-ru</span> <span class="pre">chkpt=best</span></code>, the output will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;bleu&#39;</span><span class="p">:</span> <span class="mf">26.887</span><span class="p">,</span> <span class="s1">&#39;n_obs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;runtime&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;seconds_per_sample&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;num_beams&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;early_stopping&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;info&#39;</span><span class="p">:</span> <span class="s1">&#39;pair=en-ru chkpt=best&#39;</span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>if you need to perform a parametric search in order to find the best ones that lead to the highest BLEU score, let <code class="docutils literal notranslate"><span class="pre">run_eval_search.py</span></code> to do the searching for you.</p>
<p>The script accepts the exact same arguments as <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code>, plus an additional argument <code class="docutils literal notranslate"><span class="pre">--search</span></code>. The value of <code class="docutils literal notranslate"><span class="pre">--search</span></code> is parsed, reformatted and fed to <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code> as additional args.</p>
<p>The format for the <code class="docutils literal notranslate"><span class="pre">--search</span></code> value is a simple string with hparams and colon separated values to try, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">--</span><span class="n">search</span> <span class="s2">&quot;num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false&quot;</span>
</pre></div>
</div>
<p>which will generate <code class="docutils literal notranslate"><span class="pre">12</span></code> <code class="docutils literal notranslate"><span class="pre">(2*3*2)</span></code> searches for a product of each hparam. For example the example that was just used will invoke <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code> repeatedly with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">--</span><span class="n">num_beams</span> <span class="mi">5</span> <span class="o">--</span><span class="n">length_penalty</span> <span class="mf">0.8</span> <span class="o">--</span><span class="n">early_stopping</span> <span class="n">true</span>
 <span class="o">--</span><span class="n">num_beams</span> <span class="mi">5</span> <span class="o">--</span><span class="n">length_penalty</span> <span class="mf">0.8</span> <span class="o">--</span><span class="n">early_stopping</span> <span class="n">false</span>
 <span class="p">[</span><span class="o">...</span><span class="p">]</span>
 <span class="o">--</span><span class="n">num_beams</span> <span class="mi">10</span> <span class="o">--</span><span class="n">length_penalty</span> <span class="mf">1.2</span> <span class="o">--</span><span class="n">early_stopping</span> <span class="n">false</span>
</pre></div>
</div>
<p>On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.</p>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bleu</span>  <span class="o">|</span> <span class="n">num_beams</span> <span class="o">|</span> <span class="n">length_penalty</span> <span class="o">|</span> <span class="n">early_stopping</span>
<span class="o">-----</span> <span class="o">|</span> <span class="o">---------</span> <span class="o">|</span> <span class="o">--------------</span> <span class="o">|</span> <span class="o">--------------</span>
<span class="mf">26.71</span> <span class="o">|</span>         <span class="mi">5</span> <span class="o">|</span>            <span class="mf">1.1</span> <span class="o">|</span>              <span class="mi">1</span>
<span class="mf">26.66</span> <span class="o">|</span>         <span class="mi">5</span> <span class="o">|</span>            <span class="mf">0.9</span> <span class="o">|</span>              <span class="mi">1</span>
<span class="mf">26.66</span> <span class="o">|</span>         <span class="mi">5</span> <span class="o">|</span>            <span class="mf">0.9</span> <span class="o">|</span>              <span class="mi">0</span>
<span class="mf">26.41</span> <span class="o">|</span>         <span class="mi">5</span> <span class="o">|</span>            <span class="mf">1.1</span> <span class="o">|</span>              <span class="mi">0</span>
<span class="mf">21.94</span> <span class="o">|</span>         <span class="mi">1</span> <span class="o">|</span>            <span class="mf">0.9</span> <span class="o">|</span>              <span class="mi">1</span>
<span class="mf">21.94</span> <span class="o">|</span>         <span class="mi">1</span> <span class="o">|</span>            <span class="mf">0.9</span> <span class="o">|</span>              <span class="mi">0</span>
<span class="mf">21.94</span> <span class="o">|</span>         <span class="mi">1</span> <span class="o">|</span>            <span class="mf">1.1</span> <span class="o">|</span>              <span class="mi">1</span>
<span class="mf">21.94</span> <span class="o">|</span>         <span class="mi">1</span> <span class="o">|</span>            <span class="mf">1.1</span> <span class="o">|</span>              <span class="mi">0</span>

<span class="n">Best</span> <span class="n">score</span> <span class="n">args</span><span class="p">:</span>
<span class="n">stas</span><span class="o">/</span><span class="n">wmt19</span><span class="o">-</span><span class="n">en</span><span class="o">-</span><span class="n">ru</span> <span class="n">data</span><span class="o">/</span><span class="n">en</span><span class="o">-</span><span class="n">ru</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">source</span> <span class="n">data</span><span class="o">/</span><span class="n">en</span><span class="o">-</span><span class="n">ru</span><span class="o">/</span><span class="n">test_translations</span><span class="o">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">reference_path</span> <span class="n">data</span><span class="o">/</span><span class="n">en</span><span class="o">-</span><span class="n">ru</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">target</span> <span class="o">--</span><span class="n">score_path</span> <span class="n">data</span><span class="o">/</span><span class="n">en</span><span class="o">-</span><span class="n">ru</span><span class="o">/</span><span class="n">test_bleu</span><span class="o">.</span><span class="n">json</span> <span class="o">--</span><span class="n">bs</span> <span class="mi">8</span> <span class="o">--</span><span class="n">task</span> <span class="n">translation</span> <span class="o">--</span><span class="n">num_beams</span> <span class="mi">5</span> <span class="o">--</span><span class="n">length_penalty</span> <span class="mf">1.1</span> <span class="o">--</span><span class="n">early_stopping</span> <span class="kc">True</span>
</pre></div>
</div>
<p>If you pass <code class="docutils literal notranslate"><span class="pre">--info</span> <span class="pre">&quot;some</span> <span class="pre">experiment-specific</span> <span class="pre">info&quot;</span></code> it will get printed before the results table - this is useful for scripting and multiple runs, so one can tell the different sets of results from each other.</p>
</div>
</div>
<div class="section" id="contributing">
<h3>Contributing<a class="headerlink" href="#contributing" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>follow the standard contributing guidelines and code of conduct.</p></li>
<li><p>add tests to <code class="docutils literal notranslate"><span class="pre">test_seq2seq_examples.py</span></code></p></li>
<li><p>To run only the seq2seq tests, you must be in the root of the repository and run:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest examples/seq2seq/
</pre></div>
</div>
</div>
<div class="section" id="converting-pytorch-lightning-checkpoints">
<h3>Converting pytorch-lightning checkpoints<a class="headerlink" href="#converting-pytorch-lightning-checkpoints" title="Permalink to this headline">¶</a></h3>
<p>pytorch lightning <code class="docutils literal notranslate"><span class="pre">-do_predict</span></code> often fails, after you are done training, the best way to evaluate your model is to convert it.</p>
<p>This should be done for you, with a file called <code class="docutils literal notranslate"><span class="pre">{save_dir}/best_tfmr</span></code>.</p>
<p>If that file doesn’t exist but you have a lightning <code class="docutils literal notranslate"><span class="pre">.ckpt</span></code> file, you can run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python convert_pl_checkpoint_to_hf.py PATH_TO_CKPT  randomly_initialized_hf_model_path save_dir/best_tfmr
</pre></div>
</div>
<p>Then either <code class="docutils literal notranslate"><span class="pre">run_eval</span></code> or <code class="docutils literal notranslate"><span class="pre">run_distributed_eval</span></code> with <code class="docutils literal notranslate"><span class="pre">save_dir/best_tfmr</span></code> (see previous sections)</p>
</div>
</div>
</div>
<div class="section" id="experimental-features">
<h1>Experimental Features<a class="headerlink" href="#experimental-features" title="Permalink to this headline">¶</a></h1>
<p>These features are harder to use and not always useful.</p>
<div class="section" id="dynamic-batch-size-for-mt">
<h2>Dynamic Batch Size for MT<a class="headerlink" href="#dynamic-batch-size-for-mt" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">finetune.py</span></code> has a command line arg <code class="docutils literal notranslate"><span class="pre">--max_tokens_per_batch</span></code> that allows batches to be dynamically sized.
This feature can only be used:</p>
<ul class="simple">
<li><p>with fairseq installed</p></li>
<li><p>on 1 GPU</p></li>
<li><p>without sortish sampler</p></li>
<li><p>after calling <code class="docutils literal notranslate"><span class="pre">./save_len_file.py</span> <span class="pre">$tok</span> <span class="pre">$data_dir</span></code></p></li>
</ul>
<p>For example,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./save_len_file.py Helsinki-NLP/opus-mt-en-ro  wmt_en_ro
./dynamic_bs_example.sh --max_tokens_per_batch<span class="o">=</span><span class="m">2000</span> --output_dir benchmark_dynamic_bs
</pre></div>
</div>
<p>splits <code class="docutils literal notranslate"><span class="pre">wmt_en_ro/train</span></code> into 11,197 uneven lengthed batches and can finish 1 epoch in 8 minutes on a v100.</p>
<p>For comparison,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./dynamic_bs_example.sh --sortish_sampler --train_batch_size <span class="m">48</span>
</pre></div>
</div>
<p>uses 12,723 batches of length 48 and takes slightly more time 9.5 minutes.</p>
<p>The feature is still experimental, because:</p>
<ul class="simple">
<li><p>we can make it much more robust if we have memory mapped/preprocessed datasets.</p></li>
<li><p>The speedup over sortish sampler is not that large at the moment.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>


<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Examples &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/index.html">Developer Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CONTRIBUTING.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html">Legal Information {#legal_information}</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html#trademark-information">Trademark Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/examples/pytorch/language_translation/docs/source/examples.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="examples">
<h1>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h1>
<p>In this section a few examples are put together. All of these examples work for several models, making use of the very
similar API between the different models.</p>
<p><strong>Important</strong><br />To run the latest versions of the examples, you have to install from source and install some specific requirements for the examples.
Execute the following steps in a new virtual environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/huggingface/transformers
<span class="nb">cd</span> transformers
pip install <span class="o">[</span>--editable<span class="o">]</span> .
pip install -r ./examples/requirements.txt
</pre></div>
</div>
<table border="1" class="docutils">
<thead>
<tr>
<th>Section</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#TensorFlow-2.0-Bert-models-on-GLUE">TensorFlow 2.0 models on GLUE</a></td>
<td>Examples running BERT TensorFlow 2.0 model on the GLUE tasks.</td>
</tr>
<tr>
<td><a href="#language-model-fine-tuning">Language Model fine-tuning</a></td>
<td>Fine-tuning the library models for language modeling on a text dataset. Causal language modeling for GPT/GPT-2, masked language modeling for BERT/RoBERTa.</td>
</tr>
<tr>
<td><a href="#language-generation">Language Generation</a></td>
<td>Conditional text generation using the auto-regressive models of the library: GPT, GPT-2, Transformer-XL and XLNet.</td>
</tr>
<tr>
<td><a href="#glue">GLUE</a></td>
<td>Examples running BERT/XLM/XLNet/RoBERTa on the 9 GLUE tasks. Examples feature distributed training as well as half-precision.</td>
</tr>
<tr>
<td><a href="#squad">SQuAD</a></td>
<td>Using BERT/RoBERTa/XLNet/XLM for question answering, examples with distributed training.</td>
</tr>
<tr>
<td><a href="#multiple-choice">Multiple Choice</a></td>
<td>Examples running BERT/XLNet/RoBERTa on the SWAG/RACE/ARC tasks.</td>
</tr>
<tr>
<td><a href="#named-entity-recognition">Named Entity Recognition</a></td>
<td>Using BERT for Named Entity Recognition (NER) on the CoNLL 2003 dataset, examples with distributed training.</td>
</tr>
<tr>
<td><a href="#xnli">XNLI</a></td>
<td>Examples running BERT/XLM on the XNLI benchmark.</td>
</tr>
</tbody>
</table><div class="section" id="tensorflow-2-0-bert-models-on-glue">
<h2>TensorFlow 2.0 Bert models on GLUE<a class="headerlink" href="#tensorflow-2-0-bert-models-on-glue" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_tf_glue.py"><code class="docutils literal notranslate"><span class="pre">run_tf_glue.py</span></code></a>.</p>
<p>Fine-tuning the library TensorFlow 2.0 Bert model for sequence classification on the  MRPC task of the GLUE benchmark: <a class="reference external" href="https://gluebenchmark.com/">General Language Understanding Evaluation</a>.</p>
<p>This script has an option for mixed precision (Automatic Mixed Precision / AMP) to run models on Tensor Cores (NVIDIA Volta/Turing GPUs) and future hardware and an option for XLA, which uses the XLA compiler to reduce model runtime.
Options are toggled using <code class="docutils literal notranslate"><span class="pre">USE_XLA</span></code> or <code class="docutils literal notranslate"><span class="pre">USE_AMP</span></code> variables in the script.
These options and the below benchmark are provided by &#64;tlkh.</p>
<p>Quick benchmarks from the script (no other modifications):</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>GPU</th>
<th>Mode</th>
<th>Time (2nd epoch)</th>
<th>Val Acc (3 runs)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Titan V</td>
<td>FP32</td>
<td>41s</td>
<td>0.8438/0.8281/0.8333</td>
</tr>
<tr>
<td>Titan V</td>
<td>AMP</td>
<td>26s</td>
<td>0.8281/0.8568/0.8411</td>
</tr>
<tr>
<td>V100</td>
<td>FP32</td>
<td>35s</td>
<td>0.8646/0.8359/0.8464</td>
</tr>
<tr>
<td>V100</td>
<td>AMP</td>
<td>22s</td>
<td>0.8646/0.8385/0.8411</td>
</tr>
<tr>
<td>1080 Ti</td>
<td>FP32</td>
<td>55s</td>
<td>-</td>
</tr>
</tbody>
</table><p>Mixed precision (AMP) reduces the training time considerably for the same hardware and hyper-parameters (same batch size was used).</p>
</div>
<div class="section" id="language-model-fine-tuning">
<h2>Language model fine-tuning<a class="headerlink" href="#language-model-fine-tuning" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"><code class="docutils literal notranslate"><span class="pre">run_lm_finetuning.py</span></code></a>.</p>
<p>Fine-tuning the library models for language modeling on a text dataset for GPT, GPT-2, BERT and RoBERTa (DistilBERT
to be added soon). GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa
are fine-tuned using a masked language modeling (MLM) loss.</p>
<p>Before running the following example, you should get a file that contains text on which the language model will be
fine-tuned. A good example of such text is the <a class="reference external" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-2 dataset</a>.</p>
<p>We will refer to two different files: <code class="docutils literal notranslate"><span class="pre">$TRAIN_FILE</span></code>, which contains text for training, and <code class="docutils literal notranslate"><span class="pre">$TEST_FILE</span></code>, which contains
text that will be used for evaluation.</p>
<div class="section" id="gpt-2-gpt-and-causal-language-modeling">
<h3>GPT-2/GPT and causal language modeling<a class="headerlink" href="#gpt-2-gpt-and-causal-language-modeling" title="Permalink to this headline">¶</a></h3>
<p>The following example fine-tunes GPT-2 on WikiText-2. We’re using the raw WikiText-2 (no tokens were replaced before
the tokenization). The loss here is that of causal language modeling.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
<span class="nb">export</span> <span class="nv">TEST_FILE</span><span class="o">=</span>/path/to/dataset/wiki.test.raw

python run_lm_finetuning.py <span class="se">\</span>
    --output_dir<span class="o">=</span>output <span class="se">\</span>
    --model_type<span class="o">=</span>gpt2 <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>gpt2 <span class="se">\</span>
    --do_train <span class="se">\</span>
    --train_data_file<span class="o">=</span><span class="nv">$TRAIN_FILE</span> <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --eval_data_file<span class="o">=</span><span class="nv">$TEST_FILE</span>
</pre></div>
</div>
<p>This takes about half an hour to train on a single K80 GPU and about one minute for the evaluation to run. It reaches
a score of ~20 perplexity once fine-tuned on the dataset.</p>
</div>
<div class="section" id="roberta-bert-and-masked-language-modeling">
<h3>RoBERTa/BERT and masked language modeling<a class="headerlink" href="#roberta-bert-and-masked-language-modeling" title="Permalink to this headline">¶</a></h3>
<p>The following example fine-tunes RoBERTa on WikiText-2. Here too, we’re using the raw WikiText-2. The loss is different
as BERT/RoBERTa have a bidirectional mechanism; we’re therefore using the same loss that was used during their
pre-training: masked language modeling.</p>
<p>In accordance to the RoBERTa paper, we use dynamic masking rather than static masking. The model may, therefore, converge
slightly slower (over-fitting takes more epochs).</p>
<p>We use the <code class="docutils literal notranslate"><span class="pre">--mlm</span></code> flag so that the script may change its loss function.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
<span class="nb">export</span> <span class="nv">TEST_FILE</span><span class="o">=</span>/path/to/dataset/wiki.test.raw

python run_lm_finetuning.py <span class="se">\</span>
    --output_dir<span class="o">=</span>output <span class="se">\</span>
    --model_type<span class="o">=</span>roberta <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>roberta-base <span class="se">\</span>
    --do_train <span class="se">\</span>
    --train_data_file<span class="o">=</span><span class="nv">$TRAIN_FILE</span> <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --eval_data_file<span class="o">=</span><span class="nv">$TEST_FILE</span> <span class="se">\</span>
    --mlm
</pre></div>
</div>
</div>
</div>
<div class="section" id="language-generation">
<h2>Language generation<a class="headerlink" href="#language-generation" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"><code class="docutils literal notranslate"><span class="pre">run_generation.py</span></code></a>.</p>
<p>Conditional text generation using the auto-regressive models of the library: GPT, GPT-2, Transformer-XL, XLNet, CTRL.
A similar script is used for our official demo <a class="reference external" href="https://transformer.huggingface.co">Write With Transfomer</a>, where you
can try out the different models available in the library.</p>
<p>Example usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_generation.py <span class="se">\</span>
    --model_type<span class="o">=</span>gpt2 <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>gpt2
</pre></div>
</div>
</div>
<div class="section" id="glue">
<h2>GLUE<a class="headerlink" href="#glue" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"><code class="docutils literal notranslate"><span class="pre">run_glue.py</span></code></a>.</p>
<p>Fine-tuning the library models for sequence classification on the GLUE benchmark: <a class="reference external" href="https://gluebenchmark.com/">General Language Understanding
Evaluation</a>. This script can fine-tune the following models: BERT, XLM, XLNet and RoBERTa.</p>
<p>GLUE is made up of a total of 9 different tasks. We get the following results on the dev set of the benchmark with an
uncased  BERT base model (the checkpoint <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>). All experiments ran on 8 V100 GPUs with a total train
batch size of 24. Some of these tasks have a small dataset and training can lead to high variance in the results
between different runs. We report the median on 5 runs (with different seeds) for each of the metrics.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Task</th>
<th>Metric</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoLA</td>
<td>Matthew's corr</td>
<td>48.87</td>
</tr>
<tr>
<td>SST-2</td>
<td>Accuracy</td>
<td>91.74</td>
</tr>
<tr>
<td>MRPC</td>
<td>F1/Accuracy</td>
<td>90.70/86.27</td>
</tr>
<tr>
<td>STS-B</td>
<td>Person/Spearman corr.</td>
<td>91.39/91.04</td>
</tr>
<tr>
<td>QQP</td>
<td>Accuracy/F1</td>
<td>90.79/87.66</td>
</tr>
<tr>
<td>MNLI</td>
<td>Matched acc./Mismatched acc.</td>
<td>83.70/84.83</td>
</tr>
<tr>
<td>QNLI</td>
<td>Accuracy</td>
<td>89.31</td>
</tr>
<tr>
<td>RTE</td>
<td>Accuracy</td>
<td>71.43</td>
</tr>
<tr>
<td>WNLI</td>
<td>Accuracy</td>
<td>43.66</td>
</tr>
</tbody>
</table><p>Some of these results are significantly different from the ones reported on the test set
of GLUE benchmark on the website. For QQP and WNLI, please refer to <a class="reference external" href="https://gluebenchmark.com/faq">FAQ #12</a> on the webite.</p>
<p>Before running anyone of these GLUE tasks you should download the
<a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running
<a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a>
and unpack it to some directory <code class="docutils literal notranslate"><span class="pre">$GLUE_DIR</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
<span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC

python run_glue.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 2e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
  --output_dir /tmp/<span class="nv">$TASK_NAME</span>/
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.</p>
<p>The dev set results will be present within the text file <code class="docutils literal notranslate"><span class="pre">eval_results.txt</span></code> in the specified output_dir.
In case of MNLI, since there are two separate dev sets (matched and mismatched), there will be a separate
output folder called <code class="docutils literal notranslate"><span class="pre">/tmp/MNLI-MM/</span></code> in addition to <code class="docutils literal notranslate"><span class="pre">/tmp/MNLI/</span></code>.</p>
<p>The code has not been tested with half-precision training with apex on any GLUE task apart from MRPC, MNLI,
CoLA, SST-2. The following section provides details on how to run half-precision training with MRPC. With that being
said, there shouldn’t be any issues in running half-precision training with the remaining GLUE tasks as well,
since the data processor for each task inherits from the base class DataProcessor.</p>
<div class="section" id="mrpc">
<h3>MRPC<a class="headerlink" href="#mrpc" title="Permalink to this headline">¶</a></h3>
<div class="section" id="fine-tuning-example">
<h4>Fine-tuning example<a class="headerlink" href="#fine-tuning-example" title="Permalink to this headline">¶</a></h4>
<p>The following examples fine-tune BERT on the Microsoft Research Paraphrase Corpus (MRPC) corpus and runs in less
than 10 minutes on a single K-80 and in 27 seconds (!) on single tesla V100 16GB with apex installed.</p>
<p>Before running anyone of these GLUE tasks you should download the
<a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running
<a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a>
and unpack it to some directory <code class="docutils literal notranslate"><span class="pre">$GLUE_DIR</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python run_glue.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --task_name MRPC <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/ <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 2e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
  --output_dir /tmp/mrpc_output/
</pre></div>
</div>
<p>Our test ran on a few seeds with <a class="reference external" href="https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks">the original implementation hyper-
parameters</a> gave evaluation
results between 84% and 88%.</p>
</div>
<div class="section" id="using-apex-and-mixed-precision">
<h4>Using Apex and mixed-precision<a class="headerlink" href="#using-apex-and-mixed-precision" title="Permalink to this headline">¶</a></h4>
<p>Using Apex and 16 bit precision, the fine-tuning on MRPC only takes 27 seconds. First install
<a class="reference external" href="https://github.com/NVIDIA/apex">apex</a>, then run the following example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python run_glue.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --task_name MRPC <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/ <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 2e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
  --output_dir /tmp/mrpc_output/ <span class="se">\</span>
  --fp16
</pre></div>
</div>
</div>
<div class="section" id="distributed-training">
<h4>Distributed training<a class="headerlink" href="#distributed-training" title="Permalink to this headline">¶</a></h4>
<p>Here is an example using distributed training on 8 V100 GPUs. The model used is the BERT whole-word-masking and it
reaches F1 &gt; 92 on MRPC.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python -m torch.distributed.launch <span class="se">\</span>
    --nproc_per_node <span class="m">8</span> run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-cased <span class="se">\</span>
    --task_name MRPC <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/ <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_train_batch_size <span class="m">8</span> <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir /tmp/mrpc_output/
</pre></div>
</div>
<p>Training with these hyper-parameters gave us the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8823529411764706
<span class="nv">acc_and_f1</span> <span class="o">=</span> <span class="m">0</span>.901702786377709
<span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.3418912578906332
<span class="nv">f1</span> <span class="o">=</span> <span class="m">0</span>.9210526315789473
<span class="nv">global_step</span> <span class="o">=</span> <span class="m">174</span>
<span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.07231863956341798
</pre></div>
</div>
</div>
</div>
<div class="section" id="mnli">
<h3>MNLI<a class="headerlink" href="#mnli" title="Permalink to this headline">¶</a></h3>
<p>The following example uses the BERT-large, uncased, whole-word-masking model and fine-tunes it on the MNLI task.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python -m torch.distributed.launch <span class="se">\</span>
    --nproc_per_node <span class="m">8</span> run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-cased <span class="se">\</span>
    --task_name mnli <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MNLI/ <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_train_batch_size <span class="m">8</span> <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir output_dir <span class="se">\</span>
</pre></div>
</div>
<p>The results  are the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>***** Eval results *****
  <span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8679706601466992
  <span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.4911287787382479
  <span class="nv">global_step</span> <span class="o">=</span> <span class="m">18408</span>
  <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.04755385363816904

***** Eval results *****
  <span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8747965825874695
  <span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.45516540421714036
  <span class="nv">global_step</span> <span class="o">=</span> <span class="m">18408</span>
  <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.04755385363816904
</pre></div>
</div>
</div>
</div>
<div class="section" id="multiple-choice">
<h2>Multiple Choice<a class="headerlink" href="#multiple-choice" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="#"><code class="docutils literal notranslate"><span class="pre">run_multiple_choice.py</span></code></a>.</p>
<div class="section" id="fine-tuning-on-swag">
<h3>Fine-tuning on SWAG<a class="headerlink" href="#fine-tuning-on-swag" title="Permalink to this headline">¶</a></h3>
<p>Download <a class="reference external" href="https://github.com/rowanz/swagaf/tree/master/data">swag</a> data</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#training on 4 tesla V100(16GB) GPUS</span>
<span class="nb">export</span> <span class="nv">SWAG_DIR</span><span class="o">=</span>/path/to/swag_data_dir
python ./examples/run_multiple_choice.py <span class="se">\</span>
--model_type roberta <span class="se">\</span>
--task_name swag <span class="se">\</span>
--model_name_or_path roberta-base <span class="se">\</span>
--do_train <span class="se">\</span>
--do_eval <span class="se">\</span>
--do_lower_case <span class="se">\</span>
--data_dir <span class="nv">$SWAG_DIR</span> <span class="se">\</span>
--learning_rate 5e-5 <span class="se">\</span>
--num_train_epochs <span class="m">3</span> <span class="se">\</span>
--max_seq_length <span class="m">80</span> <span class="se">\</span>
--output_dir models_bert/swag_base <span class="se">\</span>
--per_gpu_eval_batch_size<span class="o">=</span><span class="m">16</span> <span class="se">\</span>
--per_gpu_train_batch_size<span class="o">=</span><span class="m">16</span> <span class="se">\</span>
--gradient_accumulation_steps <span class="m">2</span> <span class="se">\</span>
--overwrite_output
</pre></div>
</div>
<p>Training with the defined hyper-parameters yields the following results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*****</span> <span class="n">Eval</span> <span class="n">results</span> <span class="o">*****</span>
<span class="n">eval_acc</span> <span class="o">=</span> <span class="mf">0.8338998300509847</span>
<span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.44457291918821606</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="squad">
<h2>SQuAD<a class="headerlink" href="#squad" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"><code class="docutils literal notranslate"><span class="pre">run_squad.py</span></code></a>.</p>
<div class="section" id="fine-tuning-on-squad">
<h3>Fine-tuning on SQuAD<a class="headerlink" href="#fine-tuning-on-squad" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes BERT on the SQuAD dataset. It runs in 24 min (with BERT-base) or 68 min (with BERT-large)
on a single tesla V100 16GB. The data for SQuAD can be downloaded with the following links and should be saved in a
$SQUAD_DIR directory.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json">train-v1.1.json</a></p></li>
<li><p><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json">dev-v1.1.json</a></p></li>
<li><p><a class="reference external" href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py">evaluate-v1.1.py</a></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SQUAD_DIR</span><span class="o">=</span>/path/to/SQUAD

python run_squad.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
  --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">12</span> <span class="se">\</span>
  --learning_rate 3e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">2</span>.0 <span class="se">\</span>
  --max_seq_length <span class="m">384</span> <span class="se">\</span>
  --doc_stride <span class="m">128</span> <span class="se">\</span>
  --output_dir /tmp/debug_squad/
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">f1</span> <span class="o">=</span> <span class="m">88</span>.52
<span class="nv">exact_match</span> <span class="o">=</span> <span class="m">81</span>.22
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h3>Distributed training<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Here is an example using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &gt; 93 on SQuAD:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">8</span> run_squad.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-cased <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
    --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">2</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir ../models/wwm_uncased_finetuned_squad/ <span class="se">\</span>
    --per_gpu_train_batch_size <span class="m">24</span> <span class="se">\</span>
    --gradient_accumulation_steps <span class="m">12</span>
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">f1</span> <span class="o">=</span> <span class="m">93</span>.15
<span class="nv">exact_match</span> <span class="o">=</span> <span class="m">86</span>.91
</pre></div>
</div>
<p>This fine-tuned model is available as a checkpoint under the reference
<code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking-finetuned-squad</span></code>.</p>
</div>
<div class="section" id="fine-tuning-xlnet-on-squad">
<h3>Fine-tuning XLNet on SQuAD<a class="headerlink" href="#fine-tuning-xlnet-on-squad" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes XLNet on the SQuAD dataset. See above to download the data for SQuAD .</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SQUAD_DIR</span><span class="o">=</span>/path/to/SQUAD

python /data/home/hlu/transformers/examples/run_squad.py <span class="se">\</span>
    --model_type xlnet <span class="se">\</span>
    --model_name_or_path xlnet-large-cased <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --train_file /data/home/hlu/notebooks/NLP/examples/question_answering/train-v1.1.json <span class="se">\</span>
    --predict_file /data/home/hlu/notebooks/NLP/examples/question_answering/dev-v1.1.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">2</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir ./wwm_cased_finetuned_squad/ <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">4</span>  <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">4</span>   <span class="se">\</span>
    --save_steps <span class="m">5000</span>
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s2">&quot;exact&quot;</span><span class="p">:</span> <span class="mf">85.45884578997162</span><span class="p">,</span>
<span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">92.5974600601065</span><span class="p">,</span>
<span class="s2">&quot;total&quot;</span><span class="p">:</span> <span class="mi">10570</span><span class="p">,</span>
<span class="s2">&quot;HasAns_exact&quot;</span><span class="p">:</span> <span class="mf">85.45884578997162</span><span class="p">,</span>
<span class="s2">&quot;HasAns_f1&quot;</span><span class="p">:</span> <span class="mf">92.59746006010651</span><span class="p">,</span>
<span class="s2">&quot;HasAns_total&quot;</span><span class="p">:</span> <span class="mi">10570</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="named-entity-recognition">
<h2>Named Entity Recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h2>
<p>Based on the scripts <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_ner.py"><code class="docutils literal notranslate"><span class="pre">run_ner.py</span></code></a> for Pytorch and
[<code class="docutils literal notranslate"><span class="pre">run_tf_ner.py</span></code>(https://github.com/huggingface/transformers/blob/master/examples/run_tf_ner.py)] for Tensorflow 2.
This example fine-tune Bert Multilingual on GermEval 2014 (German NER).
Details and results for the fine-tuning provided by &#64;stefan-it.</p>
<div class="section" id="data-download-and-pre-processing-steps">
<h3>Data (Download and pre-processing steps)<a class="headerlink" href="#data-download-and-pre-processing-steps" title="Permalink to this headline">¶</a></h3>
<p>Data can be obtained from the <a class="reference external" href="https://sites.google.com/site/germeval2014ner/data">GermEval 2014</a> shared task page.</p>
<p>Here are the commands for downloading and pre-processing train, dev and test datasets. The original data format has four (tab-separated) columns, in a pre-processing step only the two relevant columns (token and outer span NER annotation) are extracted:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl -L <span class="s1">&#39;https://sites.google.com/site/germeval2014ner/data/NER-de-train.tsv?attredirects=0&amp;d=1&#39;</span> <span class="se">\</span>
<span class="p">|</span> grep -v <span class="s2">&quot;^#&quot;</span> <span class="p">|</span> cut -f <span class="m">2</span>,3 <span class="p">|</span> tr <span class="s1">&#39;\t&#39;</span> <span class="s1">&#39; &#39;</span> &gt; train.txt.tmp
curl -L <span class="s1">&#39;https://sites.google.com/site/germeval2014ner/data/NER-de-dev.tsv?attredirects=0&amp;d=1&#39;</span> <span class="se">\</span>
<span class="p">|</span> grep -v <span class="s2">&quot;^#&quot;</span> <span class="p">|</span> cut -f <span class="m">2</span>,3 <span class="p">|</span> tr <span class="s1">&#39;\t&#39;</span> <span class="s1">&#39; &#39;</span> &gt; dev.txt.tmp
curl -L <span class="s1">&#39;https://sites.google.com/site/germeval2014ner/data/NER-de-test.tsv?attredirects=0&amp;d=1&#39;</span> <span class="se">\</span>
<span class="p">|</span> grep -v <span class="s2">&quot;^#&quot;</span> <span class="p">|</span> cut -f <span class="m">2</span>,3 <span class="p">|</span> tr <span class="s1">&#39;\t&#39;</span> <span class="s1">&#39; &#39;</span> &gt; test.txt.tmp
</pre></div>
</div>
<p>The GermEval 2014 dataset contains some strange “control character” tokens like <code class="docutils literal notranslate"><span class="pre">'\x96',</span> <span class="pre">'\u200e',</span> <span class="pre">'\x95',</span> <span class="pre">'\xad'</span> <span class="pre">or</span> <span class="pre">'\x80'</span></code>. One problem with these tokens is, that <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> returns an empty token for them, resulting in misaligned <code class="docutils literal notranslate"><span class="pre">InputExample</span></code>s. I wrote a script that a) filters these tokens and b) splits longer sentences into smaller ones (once the max. subtoken length is reached).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget <span class="s2">&quot;https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py&quot;</span>
</pre></div>
</div>
<p>Let’s define some variables that we need for further pre-processing steps and training the model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">MAX_LENGTH</span><span class="o">=</span><span class="m">128</span>
<span class="nb">export</span> <span class="nv">BERT_MODEL</span><span class="o">=</span>bert-base-multilingual-cased
</pre></div>
</div>
<p>Run the pre-processing script on training, dev and test datasets:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 preprocess.py train.txt.tmp <span class="nv">$BERT_MODEL</span> <span class="nv">$MAX_LENGTH</span> &gt; train.txt
python3 preprocess.py dev.txt.tmp <span class="nv">$BERT_MODEL</span> <span class="nv">$MAX_LENGTH</span> &gt; dev.txt
python3 preprocess.py test.txt.tmp <span class="nv">$BERT_MODEL</span> <span class="nv">$MAX_LENGTH</span> &gt; test.txt
</pre></div>
</div>
<p>The GermEval 2014 dataset has much more labels than CoNLL-2002/2003 datasets, so an own set of labels must be used:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat train.txt dev.txt test.txt <span class="p">|</span> cut -d <span class="s2">&quot; &quot;</span> -f <span class="m">2</span> <span class="p">|</span> grep -v <span class="s2">&quot;^</span>$<span class="s2">&quot;</span><span class="p">|</span> sort <span class="p">|</span> uniq &gt; labels.txt
</pre></div>
</div>
</div>
<div class="section" id="prepare-the-run">
<h3>Prepare the run<a class="headerlink" href="#prepare-the-run" title="Permalink to this headline">¶</a></h3>
<p>Additional environment variables must be set:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">OUTPUT_DIR</span><span class="o">=</span>germeval-model
<span class="nb">export</span> <span class="nv">BATCH_SIZE</span><span class="o">=</span><span class="m">32</span>
<span class="nb">export</span> <span class="nv">NUM_EPOCHS</span><span class="o">=</span><span class="m">3</span>
<span class="nb">export</span> <span class="nv">SAVE_STEPS</span><span class="o">=</span><span class="m">750</span>
<span class="nb">export</span> <span class="nv">SEED</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</div>
<div class="section" id="run-the-pytorch-version">
<h3>Run the Pytorch version<a class="headerlink" href="#run-the-pytorch-version" title="Permalink to this headline">¶</a></h3>
<p>To start training, just run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 run_ner.py --data_dir ./ <span class="se">\</span>
--model_type bert <span class="se">\</span>
--labels ./labels.txt <span class="se">\</span>
--model_name_or_path <span class="nv">$BERT_MODEL</span> <span class="se">\</span>
--output_dir <span class="nv">$OUTPUT_DIR</span> <span class="se">\</span>
--max_seq_length  <span class="nv">$MAX_LENGTH</span> <span class="se">\</span>
--num_train_epochs <span class="nv">$NUM_EPOCHS</span> <span class="se">\</span>
--per_gpu_train_batch_size <span class="nv">$BATCH_SIZE</span> <span class="se">\</span>
--save_steps <span class="nv">$SAVE_STEPS</span> <span class="se">\</span>
--seed <span class="nv">$SEED</span> <span class="se">\</span>
--do_train <span class="se">\</span>
--do_eval <span class="se">\</span>
--do_predict
</pre></div>
</div>
<p>If your GPU supports half-precision training, just add the <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> flag. After training, the model will be both evaluated on development and test datasets.</p>
<div class="section" id="evaluation">
<h4>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h4>
<p>Evaluation on development dataset outputs the following for our example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">10</span>/04/2019 <span class="m">00</span>:42:06 - INFO - __main__ -   ***** Eval results  *****
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:06 - INFO - __main__ -     <span class="nv">f1</span> <span class="o">=</span> <span class="m">0</span>.8623348017621146
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:06 - INFO - __main__ -     <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.07183869666975543
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:06 - INFO - __main__ -     <span class="nv">precision</span> <span class="o">=</span> <span class="m">0</span>.8467916366258111
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:06 - INFO - __main__ -     <span class="nv">recall</span> <span class="o">=</span> <span class="m">0</span>.8784592370979806
</pre></div>
</div>
<p>On the test dataset the following results could be achieved:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">10</span>/04/2019 <span class="m">00</span>:42:42 - INFO - __main__ -   ***** Eval results  *****
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:42 - INFO - __main__ -     <span class="nv">f1</span> <span class="o">=</span> <span class="m">0</span>.8614389652384803
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:42 - INFO - __main__ -     <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.07064602487454782
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:42 - INFO - __main__ -     <span class="nv">precision</span> <span class="o">=</span> <span class="m">0</span>.8604651162790697
<span class="m">10</span>/04/2019 <span class="m">00</span>:42:42 - INFO - __main__ -     <span class="nv">recall</span> <span class="o">=</span> <span class="m">0</span>.8624150210424085
</pre></div>
</div>
</div>
<div class="section" id="comparing-bert-large-cased-roberta-large-cased-and-distilbert-base-uncased">
<h4>Comparing BERT (large, cased), RoBERTa (large, cased) and DistilBERT (base, uncased)<a class="headerlink" href="#comparing-bert-large-cased-roberta-large-cased-and-distilbert-base-uncased" title="Permalink to this headline">¶</a></h4>
<p>Here is a small comparison between BERT (large, cased), RoBERTa (large, cased) and DistilBERT (base, uncased) with the same hyperparameters as specified in the <a class="reference external" href="https://huggingface.co/transformers/examples.html#named-entity-recognition">example documentation</a> (one run):</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>F-Score Dev</th>
<th>F-Score Test</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bert-large-cased</code></td>
<td>95.59</td>
<td>91.70</td>
</tr>
<tr>
<td><code>roberta-large</code></td>
<td>95.96</td>
<td>91.87</td>
</tr>
<tr>
<td><code>distilbert-base-uncased</code></td>
<td>94.34</td>
<td>90.32</td>
</tr>
</tbody>
</table></div>
</div>
<div class="section" id="run-the-tensorflow-2-version">
<h3>Run the Tensorflow 2 version<a class="headerlink" href="#run-the-tensorflow-2-version" title="Permalink to this headline">¶</a></h3>
<p>To start training, just run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 run_tf_ner.py --data_dir ./ <span class="se">\</span>
--model_type bert <span class="se">\</span>
--labels ./labels.txt <span class="se">\</span>
--model_name_or_path <span class="nv">$BERT_MODEL</span> <span class="se">\</span>
--output_dir <span class="nv">$OUTPUT_DIR</span> <span class="se">\</span>
--max_seq_length  <span class="nv">$MAX_LENGTH</span> <span class="se">\</span>
--num_train_epochs <span class="nv">$NUM_EPOCHS</span> <span class="se">\</span>
--per_device_train_batch_size <span class="nv">$BATCH_SIZE</span> <span class="se">\</span>
--save_steps <span class="nv">$SAVE_STEPS</span> <span class="se">\</span>
--seed <span class="nv">$SEED</span> <span class="se">\</span>
--do_train <span class="se">\</span>
--do_eval <span class="se">\</span>
--do_predict
</pre></div>
</div>
<p>Such as the Pytorch version, if your GPU supports half-precision training, just add the <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> flag. After training, the model will be both evaluated on development and test datasets.</p>
<div class="section" id="id2">
<h4>Evaluation<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>Evaluation on development dataset outputs the following for our example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>           precision    recall  f1-score   support

 LOCderiv     <span class="m">0</span>.7619    <span class="m">0</span>.6154    <span class="m">0</span>.6809        <span class="m">52</span>
  PERpart     <span class="m">0</span>.8724    <span class="m">0</span>.8997    <span class="m">0</span>.8858      <span class="m">4057</span>
  OTHpart     <span class="m">0</span>.9360    <span class="m">0</span>.9466    <span class="m">0</span>.9413       <span class="m">711</span>
  ORGpart     <span class="m">0</span>.7015    <span class="m">0</span>.6989    <span class="m">0</span>.7002       <span class="m">269</span>
  LOCpart     <span class="m">0</span>.7668    <span class="m">0</span>.8488    <span class="m">0</span>.8057       <span class="m">496</span>
      LOC     <span class="m">0</span>.8745    <span class="m">0</span>.9191    <span class="m">0</span>.8963       <span class="m">235</span>
 ORGderiv     <span class="m">0</span>.7723    <span class="m">0</span>.8571    <span class="m">0</span>.8125        <span class="m">91</span>
 OTHderiv     <span class="m">0</span>.4800    <span class="m">0</span>.6667    <span class="m">0</span>.5581        <span class="m">18</span>
      OTH     <span class="m">0</span>.5789    <span class="m">0</span>.6875    <span class="m">0</span>.6286        <span class="m">16</span>
 PERderiv     <span class="m">0</span>.5385    <span class="m">0</span>.3889    <span class="m">0</span>.4516        <span class="m">18</span>
      PER     <span class="m">0</span>.5000    <span class="m">0</span>.5000    <span class="m">0</span>.5000         <span class="m">2</span>
      ORG     <span class="m">0</span>.0000    <span class="m">0</span>.0000    <span class="m">0</span>.0000         <span class="m">3</span>

micro avg     <span class="m">0</span>.8574    <span class="m">0</span>.8862    <span class="m">0</span>.8715      <span class="m">5968</span>
macro avg     <span class="m">0</span>.8575    <span class="m">0</span>.8862    <span class="m">0</span>.8713      <span class="m">5968</span>
</pre></div>
</div>
<p>On the test dataset the following results could be achieved:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>           precision    recall  f1-score   support

  PERpart     <span class="m">0</span>.8847    <span class="m">0</span>.8944    <span class="m">0</span>.8896      <span class="m">9397</span>
  OTHpart     <span class="m">0</span>.9376    <span class="m">0</span>.9353    <span class="m">0</span>.9365      <span class="m">1639</span>
  ORGpart     <span class="m">0</span>.7307    <span class="m">0</span>.7044    <span class="m">0</span>.7173       <span class="m">697</span>
      LOC     <span class="m">0</span>.9133    <span class="m">0</span>.9394    <span class="m">0</span>.9262       <span class="m">561</span>
  LOCpart     <span class="m">0</span>.8058    <span class="m">0</span>.8157    <span class="m">0</span>.8107      <span class="m">1150</span>
      ORG     <span class="m">0</span>.0000    <span class="m">0</span>.0000    <span class="m">0</span>.0000         <span class="m">8</span>
 OTHderiv     <span class="m">0</span>.5882    <span class="m">0</span>.4762    <span class="m">0</span>.5263        <span class="m">42</span>
 PERderiv     <span class="m">0</span>.6571    <span class="m">0</span>.5227    <span class="m">0</span>.5823        <span class="m">44</span>
      OTH     <span class="m">0</span>.4906    <span class="m">0</span>.6667    <span class="m">0</span>.5652        <span class="m">39</span>
 ORGderiv     <span class="m">0</span>.7016    <span class="m">0</span>.7791    <span class="m">0</span>.7383       <span class="m">172</span>
 LOCderiv     <span class="m">0</span>.8256    <span class="m">0</span>.6514    <span class="m">0</span>.7282       <span class="m">109</span>
      PER     <span class="m">0</span>.0000    <span class="m">0</span>.0000    <span class="m">0</span>.0000        <span class="m">11</span>

micro avg     <span class="m">0</span>.8722    <span class="m">0</span>.8774    <span class="m">0</span>.8748     <span class="m">13869</span>
macro avg     <span class="m">0</span>.8712    <span class="m">0</span>.8774    <span class="m">0</span>.8740     <span class="m">13869</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="xnli">
<h2>XNLI<a class="headerlink" href="#xnli" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_xnli.py"><code class="docutils literal notranslate"><span class="pre">run_xnli.py</span></code></a>.</p>
<p><a class="reference external" href="https://www.nyu.edu/projects/bowman/xnli/">XNLI</a> is crowd-sourced dataset based on <a class="reference external" href="http://www.nyu.edu/projects/bowman/multinli/">MultiNLI</a>. It is an evaluation benchmark for cross-lingual text representations. Pairs of text are labeled with textual entailment annotations for 15 different languages (including both high-ressource language such as English and low-ressource languages such as Swahili).</p>
<div class="section" id="fine-tuning-on-xnli">
<h3>Fine-tuning on XNLI<a class="headerlink" href="#fine-tuning-on-xnli" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes mBERT (multi-lingual BERT) on the XNLI dataset. It runs in 106 mins
on a single tesla V100 16GB. The data for XNLI can be downloaded with the following links and should be both saved (and un-zipped) in a
<code class="docutils literal notranslate"><span class="pre">$XNLI_DIR</span></code> directory.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip">XNLI 1.0</a></p></li>
<li><p><a class="reference external" href="https://www.nyu.edu/projects/bowman/xnli/XNLI-MT-1.0.zip">XNLI-MT 1.0</a></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">XNLI_DIR</span><span class="o">=</span>/path/to/XNLI

python run_xnli.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-multilingual-cased <span class="se">\</span>
  --language de <span class="se">\</span>
  --train_language en <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --data_dir <span class="nv">$XNLI_DIR</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 5e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">2</span>.0 <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --output_dir /tmp/debug_xnli/ <span class="se">\</span>
  --save_steps -1
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results on the <strong>test</strong> set:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.7093812375249501
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
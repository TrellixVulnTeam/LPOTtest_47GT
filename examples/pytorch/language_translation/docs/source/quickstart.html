

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Quickstart &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/doclist.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../releases-info.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Quickstart</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/examples/pytorch/language_translation/docs/source/quickstart.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quickstart">
<h1>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline">¶</a></h1>
<div class="section" id="philosophy">
<h2>Philosophy<a class="headerlink" href="#philosophy" title="Permalink to this headline">¶</a></h2>
<p>Transformers is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers models.</p>
<p>The library was designed with two strong goals in mind:</p>
<ul class="simple">
<li><p>be as easy and fast to use as possible:</p>
<ul>
<li><p>we strongly limited the number of user-facing abstractions to learn, in fact there are almost no abstractions, just three standard classes required to use each model: configuration, models and tokenizer,</p></li>
<li><p>all of these classes can be initialized in a simple and unified way from pretrained instances by using a common <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> instantiation method which will take care of downloading (if needed), caching and loading the related class from a pretrained instance supplied in the library or your own saved instance.</p></li>
<li><p>as a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to extend/build-upon the library, just use regular Python/PyTorch modules and inherit from the base classes of the library to reuse functionalities like model loading/saving.</p></li>
</ul>
</li>
<li><p>provide state-of-the-art models with performances as close as possible to the original models:</p>
<ul>
<li><p>we provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture,</p></li>
<li><p>the code is usually as close to the original code base as possible which means some PyTorch code may be not as <em>pytorchic</em> as it could be as a result of being converted TensorFlow code.</p></li>
</ul>
</li>
</ul>
<p>A few other goals:</p>
<ul class="simple">
<li><p>expose the models’ internals as consistently as possible:</p>
<ul>
<li><p>we give access, using a single API to the full hidden-states and attention weights,</p></li>
<li><p>tokenizer and base model’s API are standardized to easily switch between models.</p></li>
</ul>
</li>
<li><p>incorporate a subjective selection of promising tools for fine-tuning/investigating these models:</p>
<ul>
<li><p>a simple/consistent way to add new tokens to the vocabulary and embeddings for fine-tuning,</p></li>
<li><p>simple ways to mask and prune transformer heads.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="main-concepts">
<h2>Main concepts<a class="headerlink" href="#main-concepts" title="Permalink to this headline">¶</a></h2>
<p>The library is build around three type of classes for each models:</p>
<ul class="simple">
<li><p><strong>model classes</strong> which are PyTorch models (<code class="docutils literal notranslate"><span class="pre">torch.nn.Modules</span></code>) of the 8 models architectures currently provided in the library, e.g. <code class="docutils literal notranslate"><span class="pre">BertModel</span></code></p></li>
<li><p><strong>configuration classes</strong> which store all the parameters required to build a model, e.g. <code class="docutils literal notranslate"><span class="pre">BertConfig</span></code>. You don’t always need to instantiate these your-self, in particular if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)</p></li>
<li><p><strong>tokenizer classes</strong> which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model, e.g. <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code></p></li>
</ul>
<p>All these classes can be instantiated from pretrained instances and saved locally using two methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> let you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself (currently 27 models are provided as listed <a class="reference external" href="https://huggingface.co/transformers/pretrained_models.html">here</a>) or stored locally (or on a server) by the user,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">save_pretrained()</span></code> let you save a model/configuration/tokenizer locally so that it can be reloaded using <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code>.</p></li>
</ul>
<p>We’ll finish this quickstart tour by going through a few simple quick-start examples to see how we can instantiate and use these classes. The rest of the documentation is organized in two parts:</p>
<ul class="simple">
<li><p>the <strong>MAIN CLASSES</strong> section details the common functionalities/method/attributes of the three main type of classes (configuration, model, tokenizer) plus some optimization related classes provided as utilities for training,</p></li>
<li><p>the <strong>PACKAGE REFERENCE</strong> section details all the variants of each class for each model architectures and in particular the input/output that you should expect when calling each of them.</p></li>
</ul>
</div>
<div class="section" id="quick-tour-usage">
<h2>Quick tour: Usage<a class="headerlink" href="#quick-tour-usage" title="Permalink to this headline">¶</a></h2>
<p>Here are two examples showcasing a few <code class="docutils literal notranslate"><span class="pre">Bert</span></code> and <code class="docutils literal notranslate"><span class="pre">GPT2</span></code> classes and pre-trained models.</p>
<p>See full API reference for examples for each model class.</p>
<div class="section" id="bert-example">
<h3>BERT example<a class="headerlink" href="#bert-example" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by preparing a tokenized input (a list of token embeddings indices to be fed to Bert) from a text string using <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>

<span class="c1"># OPTIONAL: if you want to have more information on what&#39;s happening under the hood, activate the logger as follows</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># Load pre-trained model tokenizer (vocabulary)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Tokenize input</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;</span>
<span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Mask a token that we will try to predict back with `BertForMaskedLM`</span>
<span class="n">masked_index</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">tokenized_text</span><span class="p">[</span><span class="n">masked_index</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;[MASK]&#39;</span>
<span class="k">assert</span> <span class="n">tokenized_text</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;who&#39;</span><span class="p">,</span> <span class="s1">&#39;was&#39;</span><span class="p">,</span> <span class="s1">&#39;jim&#39;</span><span class="p">,</span> <span class="s1">&#39;henson&#39;</span><span class="p">,</span> <span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">,</span> <span class="s1">&#39;jim&#39;</span><span class="p">,</span> <span class="s1">&#39;[MASK]&#39;</span><span class="p">,</span> <span class="s1">&#39;was&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;puppet&#39;</span><span class="p">,</span> <span class="s1">&#39;##eer&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>

<span class="c1"># Convert token to vocabulary indices</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
<span class="c1"># Define sentence A and B indices associated to 1st and 2nd sentences (see paper)</span>
<span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Convert inputs to PyTorch tensors</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
</pre></div>
</div>
<p>Let’s see how we can use <code class="docutils literal notranslate"><span class="pre">BertModel</span></code> to encode our inputs in hidden-states:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Set the model in evaluation mode to deactivate the DropOut modules</span>
<span class="c1"># This is IMPORTANT to have reproducible results during evaluation!</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># If you have a GPU, put everything on cuda</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Predict hidden states features for each layer</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># See the models docstrings for the detail of the inputs</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>
    <span class="c1"># Transformers models always output tuples.</span>
    <span class="c1"># See the models docstrings for the detail of all the outputs</span>
    <span class="c1"># In our case, the first element is the hidden state of the last layer of the Bert model</span>
    <span class="n">encoded_layers</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)</span>
<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">encoded_layers</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indexed_tokens</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
</pre></div>
</div>
<p>And how to use <code class="docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code> to predict a masked token:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># If you have a GPU, put everything on cuda</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Predict all tokens</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># confirm we were able to predict &#39;henson&#39;</span>
<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">masked_index</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">predicted_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">predicted_index</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">predicted_token</span> <span class="o">==</span> <span class="s1">&#39;henson&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="openai-gpt-2">
<h3>OpenAI GPT-2<a class="headerlink" href="#openai-gpt-2" title="Permalink to this headline">¶</a></h3>
<p>Here is a quick-start example using <code class="docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code> and <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> class with OpenAI’s pre-trained model to predict the next token from a text prompt.</p>
<p>First let’s prepare a tokenized input from our text string using <code class="docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>

<span class="c1"># OPTIONAL: if you want to have more information on what&#39;s happening, activate the logger as follows</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># Load pre-trained model tokenizer (vocabulary)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Encode a text inputs</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Who was Jim Henson ? Jim Henson was a&quot;</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Convert indexed tokens in a PyTorch tensor</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
</pre></div>
</div>
<p>Let’s see how to use <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> to generate the next token following our text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Set the model in evaluation mode to deactivate the DropOut modules</span>
<span class="c1"># This is IMPORTANT to have reproducible results during evaluation!</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># If you have a GPU, put everything on cuda</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Predict all tokens</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># get the predicted next sub-word (in our case, the word &#39;man&#39;)</span>
<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">predicted_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">indexed_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">predicted_index</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">predicted_text</span> <span class="o">==</span> <span class="s1">&#39;Who was Jim Henson? Jim Henson was a man&#39;</span>
</pre></div>
</div>
<p>Examples for each model class of each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the <a class="reference external" href="#documentation">documentation</a>.</p>
<div class="section" id="using-the-past">
<h4>Using the past<a class="headerlink" href="#using-the-past" title="Permalink to this headline">¶</a></h4>
<p>GPT-2 as well as some other models (GPT, XLNet, Transfo-XL, CTRL) make use of a <code class="docutils literal notranslate"><span class="pre">past</span></code> or <code class="docutils literal notranslate"><span class="pre">mems</span></code> attribute which can be used to prevent re-computing the key/value pairs when using sequential decoding. It is useful when generating sequences as a big part of the attention mechanism benefits from previous computations.</p>
<p>Here is a fully-working example using the <code class="docutils literal notranslate"><span class="pre">past</span></code> with <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> and argmax decoding (which should only be used as an example, as argmax decoding introduces a lot of repetition):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;The Manhattan bridge&quot;</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">generated</span><span class="p">])</span>
<span class="n">past</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">generated</span> <span class="o">+=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</pre></div>
</div>
<p>The model only requires a single token as input as all the previous tokens’ key/value pairs are contained in the <code class="docutils literal notranslate"><span class="pre">past</span></code>.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
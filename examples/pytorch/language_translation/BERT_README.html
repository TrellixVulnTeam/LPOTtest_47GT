

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Features &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/doclist.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases-info.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Features</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/examples/pytorch/language_translation/BERT_README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p align="center">
    <br>
    <img src="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png" width="400"/>
    <br>
<p>
<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers">
        <img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/master">
    </a>
    <a href="https://github.com/huggingface/transformers/blob/master/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
    </a>
    <a href="https://huggingface.co/transformers/index.html">
        <img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&down_message=offline&up_message=online">
    </a>
    <a href="https://github.com/huggingface/transformers/releases">
        <img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg">
    </a>
</p><h3 align="center">
<p>State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch
</h3><p>🤗 Transformers (formerly known as <code class="docutils literal notranslate"><span class="pre">pytorch-transformers</span></code> and <code class="docutils literal notranslate"><span class="pre">pytorch-pretrained-bert</span></code>) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.</p>
<div class="section" id="features">
<h1>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>As easy to use as pytorch-transformers</p></li>
<li><p>As powerful and concise as Keras</p></li>
<li><p>High performance on NLU and NLG tasks</p></li>
<li><p>Low barrier to entry for educators and practitioners</p></li>
</ul>
<p>State-of-the-art NLP for everyone</p>
<ul class="simple">
<li><p>Deep learning researchers</p></li>
<li><p>Hands-on practitioners</p></li>
<li><p>AI/ML/NLP teachers and educators</p></li>
</ul>
<p>Lower compute costs, smaller carbon footprint</p>
<ul class="simple">
<li><p>Researchers can share trained models instead of always retraining</p></li>
<li><p>Practitioners can reduce compute time and production costs</p></li>
<li><p>10 architectures with over 30 pretrained models, some in more than 100 languages</p></li>
</ul>
<p>Choose the right framework for every part of a model’s lifetime</p>
<ul class="simple">
<li><p>Train state-of-the-art models in 3 lines of code</p></li>
<li><p>Deep interoperability between TensorFlow 2.0 and PyTorch models</p></li>
<li><p>Move a single model between TF2.0/PyTorch frameworks at will</p></li>
<li><p>Seamlessly pick the right framework for training, evaluation, production</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>Section</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#installation">Installation</a></td>
<td>How to install the package</td>
</tr>
<tr>
<td><a href="#model-architectures">Model architectures</a></td>
<td>Architectures (with pretrained weights)</td>
</tr>
<tr>
<td><a href="#online-demo">Online demo</a></td>
<td>Experimenting with this repo’s text generation capabilities</td>
</tr>
<tr>
<td><a href="#quick-tour">Quick tour: Usage</a></td>
<td>Tokenizers &amp; models usage: Bert and GPT-2</td>
</tr>
<tr>
<td><a href="#Quick-tour-TF-20-training-and-PyTorch-interoperability">Quick tour: TF 2.0 and PyTorch </a></td>
<td>Train a TF 2.0 model in 10 lines of code, load it in PyTorch</td>
</tr>
<tr>
<td><a href="#quick-tour-of-the-fine-tuningusage-scripts">Quick tour: Fine-tuning/usage scripts</a></td>
<td>Using provided scripts: GLUE, SQuAD and Text generation</td>
</tr>
<tr>
<td><a href="#Migrating-from-pytorch-transformers-to-transformers">Migrating from pytorch-transformers to transformers</a></td>
<td>Migrating your code from pytorch-transformers to transformers</td>
</tr>
<tr>
<td><a href="#Migrating-from-pytorch-pretrained-bert-to-transformers">Migrating from pytorch-pretrained-bert to pytorch-transformers</a></td>
<td>Migrating your code from pytorch-pretrained-bert to transformers</td>
</tr>
<tr>
<td>[Documentation]<a href="https://huggingface.co/transformers/v2.2.0">(v2.2.0/v2.2.1)</a> <a href="https://huggingface.co/transformers/v2.1.1">(v2.1.1)</a> <a href="https://huggingface.co/transformers/v2.0.0">(v2.0.0)</a> <a href="https://huggingface.co/transformers/v1.2.0">(v1.2.0)</a> <a href="https://huggingface.co/transformers/v1.1.0">(v1.1.0)</a> <a href="https://huggingface.co/transformers/v1.0.0">(v1.0.0)</a> <a href="https://huggingface.co/transformers">(master)</a></td>
<td>Full API documentation and more</td>
</tr>
</tbody>
</table></div>
<div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1</p>
<div class="section" id="with-pip">
<h2>With pip<a class="headerlink" href="#with-pip" title="Permalink to this headline">¶</a></h2>
<p>First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to <a class="reference external" href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available">TensorFlow installation page</a> and/or <a class="reference external" href="https://pytorch.org/get-started/locally/#start-locally">PyTorch installation page</a> regarding the specific install command for your platform.</p>
<p>When TensorFlow 2.0 and/or PyTorch has been installed, 🤗 Transformers can be installed using pip as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers
</pre></div>
</div>
</div>
<div class="section" id="from-source">
<h2>From source<a class="headerlink" href="#from-source" title="Permalink to this headline">¶</a></h2>
<p>Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to <a class="reference external" href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available">TensorFlow installation page</a> and/or <a class="reference external" href="https://pytorch.org/get-started/locally/#start-locally">PyTorch installation page</a> regarding the specific install command for your platform.</p>
<p>When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install <span class="o">[</span>--editable<span class="o">]</span> .
</pre></div>
</div>
</div>
<div class="section" id="run-the-examples">
<h2>Run the examples<a class="headerlink" href="#run-the-examples" title="Permalink to this headline">¶</a></h2>
<p>Examples are included in the repository but are not shipped with the library.
Therefore, in order to run the latest versions of the examples you also need to install from source. To do so, create a new virtual environment and follow these steps:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/huggingface/transformers
<span class="nb">cd</span> transformers
pip install <span class="o">[</span>--editable<span class="o">]</span> .
</pre></div>
</div>
</div>
<div class="section" id="tests">
<h2>Tests<a class="headerlink" href="#tests" title="Permalink to this headline">¶</a></h2>
<p>A series of tests are included for the library and the example scripts. Library tests can be found in the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/transformers/tests">tests folder</a> and examples tests in the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">examples folder</a>.</p>
<p>These tests can be run using <code class="docutils literal notranslate"><span class="pre">unittest</span></code> or <code class="docutils literal notranslate"><span class="pre">pytest</span></code> (install pytest if needed with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pytest</span></code>).</p>
<p>Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.</p>
<p>You can run the tests from the root of the cloned repository with the commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m unittest discover -s transformers/tests -p <span class="s2">&quot;*test.py&quot;</span> -t .
python -m unittest discover -s examples -p <span class="s2">&quot;*test.py&quot;</span> -t examples
</pre></div>
</div>
<p>or</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/
</pre></div>
</div>
<p>By default, slow tests are skipped. Set the <code class="docutils literal notranslate"><span class="pre">RUN_SLOW</span></code> environment variable to <code class="docutils literal notranslate"><span class="pre">yes</span></code> to run them.</p>
</div>
<div class="section" id="do-you-want-to-run-a-transformer-model-on-a-mobile-device">
<h2>Do you want to run a Transformer model on a mobile device?<a class="headerlink" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device" title="Permalink to this headline">¶</a></h2>
<p>You should check out our <a class="reference external" href="https://github.com/huggingface/swift-coreml-transformers"><code class="docutils literal notranslate"><span class="pre">swift-coreml-transformers</span></code></a> repo.</p>
<p>It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains <code class="docutils literal notranslate"><span class="pre">GPT-2</span></code>, <code class="docutils literal notranslate"><span class="pre">DistilGPT-2</span></code>, <code class="docutils literal notranslate"><span class="pre">BERT</span></code>, and <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code>) to CoreML models that run on iOS devices.</p>
<p>At some point in the future, you’ll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!</p>
</div>
</div>
<div class="section" id="model-architectures">
<h1>Model architectures<a class="headerlink" href="#model-architectures" title="Permalink to this headline">¶</a></h1>
<p>🤗 Transformers currently provides 10 NLU/NLG architectures:</p>
<ol class="simple">
<li><p><strong><a class="reference external" href="https://github.com/google-research/bert">BERT</a></strong> (from Google) released with the paper <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/openai/finetune-transformer-lm">GPT</a></strong> (from OpenAI) released with the paper <a class="reference external" href="https://blog.openai.com/language-unsupervised/">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.</p></li>
<li><p><strong><a class="reference external" href="https://blog.openai.com/better-language-models/">GPT-2</a></strong> (from OpenAI) released with the paper <a class="reference external" href="https://blog.openai.com/better-language-models/">Language Models are Unsupervised Multitask Learners</a> by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/kimiyoung/transformer-xl">Transformer-XL</a></strong> (from Google/CMU) released with the paper <a class="reference external" href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/zihangdai/xlnet/">XLNet</a></strong> (from Google/CMU) released with the paper <a class="reference external" href="https://arxiv.org/abs/1906.08237">​XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/facebookresearch/XLM/">XLM</a></strong> (from Facebook) released together with the paper <a class="reference external" href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/pytorch/fairseq/tree/master/examples/roberta">RoBERTa</a></strong> (from Facebook), released together with the paper a <a class="reference external" href="https://arxiv.org/abs/1907.11692">Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/distillation">DistilBERT</a></strong> (from HuggingFace), released together with the paper <a class="reference external" href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/distillation">DistilGPT2</a>, RoBERTa into <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/distillation">DistilRoBERTa</a>, Multilingual BERT into <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/distillation">DistilmBERT</a> and a German version of DistilBERT.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/salesforce/ctrl/">CTRL</a></strong> (from Salesforce) released with the paper <a class="reference external" href="https://arxiv.org/abs/1909.05858">CTRL: A Conditional Transformer Language Model for Controllable Generation</a> by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.</p></li>
<li><p><strong><a class="reference external" href="https://camembert-model.fr">CamemBERT</a></strong> (from Inria/Facebook/Sorbonne) released with the paper <a class="reference external" href="https://arxiv.org/abs/1911.03894">CamemBERT: a Tasty French Language Model</a> by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.</p></li>
<li><p><strong><a class="reference external" href="https://github.com/google-research/ALBERT">ALBERT</a></strong> (from Google Research and the Toyota Technological Institute at Chicago) released with the paper <a class="reference external" href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.</p></li>
<li><p>Want to contribute a new model? We have added a <strong>detailed guide and templates</strong> to guide you in the process of adding a new model. You can find them in the <a class="reference external" href="examples/pytorch/language_translation/./templates"><code class="docutils literal notranslate"><span class="pre">templates</span></code></a> folder of the repository. Be sure to check the <a class="reference internal" href="CONTRIBUTING.html"><span class="doc">contributing guidelines</span></a> and contact the maintainers or open an issue to collect feedbacks before starting your PR.</p></li>
</ol>
<p>These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the <a class="reference external" href="https://huggingface.co/transformers/examples.html">documentation</a>.</p>
</div>
<div class="section" id="online-demo">
<h1>Online demo<a class="headerlink" href="#online-demo" title="Permalink to this headline">¶</a></h1>
<p><strong><a class="reference external" href="https://transformer.huggingface.co">Write With Transformer</a></strong>, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo’s text generation capabilities.
You can use it to experiment with completions generated by <code class="docutils literal notranslate"><span class="pre">GPT2Model</span></code>, <code class="docutils literal notranslate"><span class="pre">TransfoXLModel</span></code>, and <code class="docutils literal notranslate"><span class="pre">XLNetModel</span></code>.</p>
<blockquote>
<div><p>“🦄 Write with transformer is to writing what calculators are to calculus.”</p>
</div></blockquote>
<p><img alt="write_with_transformer" src="https://transformer.huggingface.co/front/assets/thumbnail-large.png" /></p>
</div>
<div class="section" id="quick-tour">
<h1>Quick tour<a class="headerlink" href="#quick-tour" title="Permalink to this headline">¶</a></h1>
<p>Let’s do a very quick overview of the model architectures in 🤗 Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the <a class="reference external" href="https://huggingface.co/transformers/">full documentation</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Transformers has a unified API</span>
<span class="c1"># for 8 transformer architectures and 30 pretrained weights.</span>
<span class="c1">#          Model          | Tokenizer          | Pretrained weights shortcut</span>
<span class="n">MODELS</span> <span class="o">=</span> <span class="p">[(</span><span class="n">BertModel</span><span class="p">,</span>       <span class="n">BertTokenizer</span><span class="p">,</span>       <span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">OpenAIGPTModel</span><span class="p">,</span>  <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span>  <span class="s1">&#39;openai-gpt&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">GPT2Model</span><span class="p">,</span>       <span class="n">GPT2Tokenizer</span><span class="p">,</span>       <span class="s1">&#39;gpt2&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">CTRLModel</span><span class="p">,</span>       <span class="n">CTRLTokenizer</span><span class="p">,</span>       <span class="s1">&#39;ctrl&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">TransfoXLModel</span><span class="p">,</span>  <span class="n">TransfoXLTokenizer</span><span class="p">,</span>  <span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">XLNetModel</span><span class="p">,</span>      <span class="n">XLNetTokenizer</span><span class="p">,</span>      <span class="s1">&#39;xlnet-base-cased&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">XLMModel</span><span class="p">,</span>        <span class="n">XLMTokenizer</span><span class="p">,</span>        <span class="s1">&#39;xlm-mlm-enfr-1024&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">DistilBertModel</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">),</span>
          <span class="p">(</span><span class="n">RobertaModel</span><span class="p">,</span>    <span class="n">RobertaTokenizer</span><span class="p">,</span>    <span class="s1">&#39;roberta-base&#39;</span><span class="p">)]</span>

<span class="c1"># To use TensorFlow 2.0 versions of the models, simply prefix the class names with &#39;TF&#39;, e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`</span>

<span class="c1"># Let&#39;s encode some text in a sequence of hidden-states using each model:</span>
<span class="k">for</span> <span class="n">model_class</span><span class="p">,</span> <span class="n">tokenizer_class</span><span class="p">,</span> <span class="n">pretrained_weights</span> <span class="ow">in</span> <span class="n">MODELS</span><span class="p">:</span>
    <span class="c1"># Load pretrained model/tokenizer</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>

    <span class="c1"># Encode text</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Here is some text to encode&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)])</span>  <span class="c1"># Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Models outputs are now tuples</span>

<span class="c1"># Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.</span>
<span class="n">BERT_MODEL_CLASSES</span> <span class="o">=</span> <span class="p">[</span><span class="n">BertModel</span><span class="p">,</span> <span class="n">BertForPreTraining</span><span class="p">,</span> <span class="n">BertForMaskedLM</span><span class="p">,</span> <span class="n">BertForNextSentencePrediction</span><span class="p">,</span>
                      <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertForTokenClassification</span><span class="p">,</span> <span class="n">BertForQuestionAnswering</span><span class="p">]</span>

<span class="c1"># All the classes for an architecture can be initiated from pretrained weights for this architecture</span>
<span class="c1"># Note that additional weights added for fine-tuning are only initialized</span>
<span class="c1"># and need to be trained on the down-stream task</span>
<span class="n">pretrained_weights</span> <span class="o">=</span> <span class="s1">&#39;bert-base-uncased&#39;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>
<span class="k">for</span> <span class="n">model_class</span> <span class="ow">in</span> <span class="n">BERT_MODEL_CLASSES</span><span class="p">:</span>
    <span class="c1"># Load pretrained model/tokenizer</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>

    <span class="c1"># Models can return full list of hidden-states &amp; attentions weights at each layer</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">,</span>
                                        <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                        <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Let&#39;s see all hidden-states and attentions on this text&quot;</span><span class="p">)])</span>
    <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

    <span class="c1"># Models are compatible with Torchscript</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">,</span> <span class="n">torchscript</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">input_ids</span><span class="p">,))</span>

    <span class="c1"># Simple serialization for models and tokenizers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;./directory/to/save/&#39;</span><span class="p">)</span>  <span class="c1"># save</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./directory/to/save/&#39;</span><span class="p">)</span>  <span class="c1"># re-load</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;./directory/to/save/&#39;</span><span class="p">)</span>  <span class="c1"># save</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./directory/to/save/&#39;</span><span class="p">)</span>  <span class="c1"># re-load</span>

    <span class="c1"># SOTA examples for GLUE, SQUAD, text generation...</span>
</pre></div>
</div>
</div>
<div class="section" id="quick-tour-tf-2-0-training-and-pytorch-interoperability">
<h1>Quick tour TF 2.0 training and PyTorch interoperability<a class="headerlink" href="#quick-tour-tf-2-0-training-and-pytorch-interoperability" title="Permalink to this headline">¶</a></h1>
<p>Let’s do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with 🤗 Transformers and then loaded in PyTorch for fast inspection/tests.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Load dataset, tokenizer, model from pretrained model/vocabulary</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">tensorflow_datasets</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;glue/mrpc&#39;</span><span class="p">)</span>

<span class="c1"># Prepare dataset for GLUE as a tf.data.Dataset instance</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">glue_convert_examples_to_features</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s1">&#39;mrpc&#39;</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">glue_convert_examples_to_features</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s1">&#39;mrpc&#39;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">valid_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule </span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">clipnorm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span>

<span class="c1"># Train and evaluate using tf.keras.Model.fit()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">115</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">validation_steps</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Load the TensorFlow model in PyTorch for inspection</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;./save/&#39;</span><span class="p">)</span>
<span class="n">pytorch_model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./save/&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Quickly test a few predictions - MRPC is a paraphrasing task, let&#39;s see if our model learned the task</span>
<span class="n">sentence_0</span> <span class="o">=</span> <span class="s2">&quot;This research was consistent with his findings.&quot;</span>
<span class="n">sentence_1</span> <span class="o">=</span> <span class="s2">&quot;His findings were compatible with this research.&quot;</span>
<span class="n">sentence_2</span> <span class="o">=</span> <span class="s2">&quot;His findings were not compatible with this research.&quot;</span>
<span class="n">inputs_1</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">sentence_0</span><span class="p">,</span> <span class="n">sentence_1</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">inputs_2</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">sentence_0</span><span class="p">,</span> <span class="n">sentence_2</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="n">pred_1</span> <span class="o">=</span> <span class="n">pytorch_model</span><span class="p">(</span><span class="n">inputs_1</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">inputs_1</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">pred_2</span> <span class="o">=</span> <span class="n">pytorch_model</span><span class="p">(</span><span class="n">inputs_2</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">inputs_2</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;sentence_1 is&quot;</span><span class="p">,</span> <span class="s2">&quot;a paraphrase&quot;</span> <span class="k">if</span> <span class="n">pred_1</span> <span class="k">else</span> <span class="s2">&quot;not a paraphrase&quot;</span><span class="p">,</span> <span class="s2">&quot;of sentence_0&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;sentence_2 is&quot;</span><span class="p">,</span> <span class="s2">&quot;a paraphrase&quot;</span> <span class="k">if</span> <span class="n">pred_2</span> <span class="k">else</span> <span class="s2">&quot;not a paraphrase&quot;</span><span class="p">,</span> <span class="s2">&quot;of sentence_0&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="quick-tour-of-the-fine-tuning-usage-scripts">
<h1>Quick tour of the fine-tuning/usage scripts<a class="headerlink" href="#quick-tour-of-the-fine-tuning-usage-scripts" title="Permalink to this headline">¶</a></h1>
<p><strong>Important</strong><br />Before running the fine-tuning scripts, please read the
<a class="reference external" href="#run-the-examples">instructions</a> on how to
setup your environment to run the examples.</p>
<p>The library comprises several example scripts with SOTA performances for NLU and NLG tasks:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">run_glue.py</span></code>: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (<em>sequence-level classification</em>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_squad.py</span></code>: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (<em>token-level classification</em>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_generation.py</span></code>: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation</p></li>
<li><p>other model-specific examples (see the documentation).</p></li>
</ul>
<p>Here are three quick usage examples for these scripts:</p>
<div class="section" id="run-glue-py-fine-tuning-on-glue-tasks-for-sequence-classification">
<h2><code class="docutils literal notranslate"><span class="pre">run_glue.py</span></code>: Fine-tuning on GLUE tasks for sequence classification<a class="headerlink" href="#run-glue-py-fine-tuning-on-glue-tasks-for-sequence-classification" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference external" href="https://gluebenchmark.com/">General Language Understanding Evaluation (GLUE) benchmark</a> is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.</p>
<p>Before running anyone of these GLUE tasks you should download the
<a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running
<a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a>
and unpack it to some directory <code class="docutils literal notranslate"><span class="pre">$GLUE_DIR</span></code>.</p>
<p>You should also install the additional packages required by the examples:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip install -r ./examples/requirements.txt
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
<span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC

python ./examples/run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-uncased <span class="se">\</span>
    --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir /tmp/<span class="nv">$TASK_NAME</span>/
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.</p>
<p>The dev set results will be present within the text file ‘eval_results.txt’ in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called ‘/tmp/MNLI-MM/’ in addition to ‘/tmp/MNLI/’.</p>
<div class="section" id="fine-tuning-xlnet-model-on-the-sts-b-regression-task">
<h3>Fine-tuning XLNet model on the STS-B regression task<a class="headerlink" href="#fine-tuning-xlnet-model-on-the-sts-b-regression-task" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python ./examples/run_glue.py <span class="se">\</span>
    --model_type xlnet <span class="se">\</span>
    --model_name_or_path xlnet-large-cased <span class="se">\</span>
    --do_train  <span class="se">\</span>
    --do_eval   <span class="se">\</span>
    --task_name<span class="o">=</span>sts-b     <span class="se">\</span>
    --data_dir<span class="o">=</span><span class="si">${</span><span class="nv">GLUE_DIR</span><span class="si">}</span>/STS-B  <span class="se">\</span>
    --output_dir<span class="o">=</span>./proc_data/sts-b-110   <span class="se">\</span>
    --max_seq_length<span class="o">=</span><span class="m">128</span>   <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --gradient_accumulation_steps<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
    --max_steps<span class="o">=</span><span class="m">1200</span>  <span class="se">\</span>
    --model_name<span class="o">=</span>xlnet-large-cased   <span class="se">\</span>
    --overwrite_output_dir   <span class="se">\</span>
    --overwrite_cache <span class="se">\</span>
    --warmup_steps<span class="o">=</span><span class="m">120</span>
</pre></div>
</div>
<p>On this machine we thus have a batch size of 32, please increase <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span></code> to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of <code class="docutils literal notranslate"><span class="pre">+0.917</span></code> on the development set.</p>
</div>
<div class="section" id="fine-tuning-bert-model-on-the-mrpc-classification-task">
<h3>Fine-tuning Bert model on the MRPC classification task<a class="headerlink" href="#fine-tuning-bert-model-on-the-mrpc-classification-task" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &gt; 92.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node <span class="m">8</span> ./examples/run_glue.py   <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --task_name MRPC <span class="se">\</span>
    --do_train   <span class="se">\</span>
    --do_eval   <span class="se">\</span>
    --do_lower_case   <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/   <span class="se">\</span>
    --max_seq_length <span class="m">128</span>   <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --learning_rate 2e-5   <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0  <span class="se">\</span>
    --output_dir /tmp/mrpc_output/ <span class="se">\</span>
    --overwrite_output_dir   <span class="se">\</span>
    --overwrite_cache <span class="se">\</span>
</pre></div>
</div>
<p>Training with these hyper-parameters gave us the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8823529411764706
  <span class="nv">acc_and_f1</span> <span class="o">=</span> <span class="m">0</span>.901702786377709
  <span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.3418912578906332
  <span class="nv">f1</span> <span class="o">=</span> <span class="m">0</span>.9210526315789473
  <span class="nv">global_step</span> <span class="o">=</span> <span class="m">174</span>
  <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.07231863956341798
</pre></div>
</div>
</div>
</div>
<div class="section" id="run-squad-py-fine-tuning-on-squad-for-question-answering">
<h2><code class="docutils literal notranslate"><span class="pre">run_squad.py</span></code>: Fine-tuning on SQuAD for question-answering<a class="headerlink" href="#run-squad-py-fine-tuning-on-squad-for-question-answering" title="Permalink to this headline">¶</a></h2>
<p>This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &gt; 93 on SQuAD:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">8</span> ./examples/run_squad.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
    --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">2</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir ../models/wwm_uncased_finetuned_squad/ <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">3</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">3</span>   <span class="se">\</span>
</pre></div>
</div>
<p>Training with these hyper-parameters gave us the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python <span class="nv">$SQUAD_DIR</span>/evaluate-v1.1.py <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
<span class="o">{</span><span class="s2">&quot;exact_match&quot;</span>: <span class="m">86</span>.91579943235573, <span class="s2">&quot;f1&quot;</span>: <span class="m">93</span>.1532499015869<span class="o">}</span>
</pre></div>
</div>
<p>This is the model provided as <code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking-finetuned-squad</span></code>.</p>
</div>
<div class="section" id="run-generation-py-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet">
<h2><code class="docutils literal notranslate"><span class="pre">run_generation.py</span></code>: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet<a class="headerlink" href="#run-generation-py-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet" title="Permalink to this headline">¶</a></h2>
<p>A conditional generation script is also included to generate text from a prompt.
The generation script includes the <a class="reference external" href="https://github.com/rusiaaman/XLNet-gen#methodology">tricks</a> proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).</p>
<p>Here is how to run the script with the small version of OpenAI GPT-2 model:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ./examples/run_generation.py <span class="se">\</span>
    --model_type<span class="o">=</span>gpt2 <span class="se">\</span>
    --length<span class="o">=</span><span class="m">20</span> <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>gpt2 <span class="se">\</span>
</pre></div>
</div>
<p>and from the Salesforce CTRL model:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ./examples/run_generation.py <span class="se">\</span>
    --model_type<span class="o">=</span>ctrl <span class="se">\</span>
    --length<span class="o">=</span><span class="m">20</span> <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>ctrl <span class="se">\</span>
    --temperature<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
    --repetition_penalty<span class="o">=</span><span class="m">1</span>.2 <span class="se">\</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="migrating-from-pytorch-transformers-to-transformers">
<h1>Migrating from pytorch-transformers to transformers<a class="headerlink" href="#migrating-from-pytorch-transformers-to-transformers" title="Permalink to this headline">¶</a></h1>
<p>Here is a quick summary of what you should take care of when migrating from <code class="docutils literal notranslate"><span class="pre">pytorch-transformers</span></code> to <code class="docutils literal notranslate"><span class="pre">transformers</span></code>.</p>
<div class="section" id="positional-order-of-some-models-keywords-inputs-attention-mask-token-type-ids-changed">
<h2>Positional order of some models’ keywords inputs (<code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code>…) changed<a class="headerlink" href="#positional-order-of-some-models-keywords-inputs-attention-mask-token-type-ids-changed" title="Permalink to this headline">¶</a></h2>
<p>To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models <strong>keywords inputs</strong> (<code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code>…) has been changed.</p>
<p>If you used to call the models with keyword names for keyword arguments, e.g. <code class="docutils literal notranslate"><span class="pre">model(inputs_ids,</span> <span class="pre">attention_mask=attention_mask,</span> <span class="pre">token_type_ids=token_type_ids)</span></code>, this should not cause any change.</p>
<p>If you used to call the models with positional inputs for keyword arguments, e.g. <code class="docutils literal notranslate"><span class="pre">model(inputs_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids)</span></code>, you may have to double check the exact order of input arguments.</p>
</div>
</div>
<div class="section" id="migrating-from-pytorch-pretrained-bert-to-transformers">
<h1>Migrating from pytorch-pretrained-bert to transformers<a class="headerlink" href="#migrating-from-pytorch-pretrained-bert-to-transformers" title="Permalink to this headline">¶</a></h1>
<p>Here is a quick summary of what you should take care of when migrating from <code class="docutils literal notranslate"><span class="pre">pytorch-pretrained-bert</span></code> to <code class="docutils literal notranslate"><span class="pre">transformers</span></code>.</p>
<div class="section" id="models-always-output-tuples">
<h2>Models always output <code class="docutils literal notranslate"><span class="pre">tuples</span></code><a class="headerlink" href="#models-always-output-tuples" title="Permalink to this headline">¶</a></h2>
<p>The main breaking change when migrating from <code class="docutils literal notranslate"><span class="pre">pytorch-pretrained-bert</span></code> to <code class="docutils literal notranslate"><span class="pre">transformers</span></code> is that every model’s forward method always outputs a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> with various elements depending on the model and the configuration parameters.</p>
<p>The exact content of the tuples for each model is detailed in the models’ docstrings and the <a class="reference external" href="https://huggingface.co/transformers/">documentation</a>.</p>
<p>In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in <code class="docutils literal notranslate"><span class="pre">pytorch-pretrained-bert</span></code>.</p>
<p>Here is a <code class="docutils literal notranslate"><span class="pre">pytorch-pretrained-bert</span></code> to <code class="docutils literal notranslate"><span class="pre">transformers</span></code> conversion example for a <code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code> classification model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s load our model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># If you used to have this line in pytorch-pretrained-bert:</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Now just use this line in transformers to extract the loss from the output tuple:</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># In transformers you can also have access to the logits:</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">attentions</span> <span class="o">=</span> <span class="n">outputs</span>
</pre></div>
</div>
</div>
<div class="section" id="using-hidden-states">
<h2>Using hidden states<a class="headerlink" href="#using-hidden-states" title="Permalink to this headline">¶</a></h2>
<p>By enabling the configuration option <code class="docutils literal notranslate"><span class="pre">output_hidden_states</span></code>, it was possible to retrieve the last hidden states of the encoder. In <code class="docutils literal notranslate"><span class="pre">pytorch-transformers</span></code> as well as <code class="docutils literal notranslate"><span class="pre">transformers</span></code> the return value has changed slightly: <code class="docutils literal notranslate"><span class="pre">all_hidden_states</span></code> now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.</p>
</div>
<div class="section" id="serialization">
<h2>Serialization<a class="headerlink" href="#serialization" title="Permalink to this headline">¶</a></h2>
<p>Breaking change in the <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method:</p>
<ol class="simple">
<li><p>Models are now set in evaluation mode by default when instantiated with the <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method. To train them, don’t forget to set them back in training mode (<code class="docutils literal notranslate"><span class="pre">model.train()</span></code>) to activate the dropout modules.</p></li>
<li><p>The additional <code class="docutils literal notranslate"><span class="pre">*input</span></code> and <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> arguments supplied to the <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method used to be directly passed to the underlying model’s class <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous <code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code> examples. We are working on a way to mitigate this breaking change in <a class="reference external" href="https://github.com/huggingface/transformers/pull/866">#866</a> by forwarding the the model’s <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.</p></li>
</ol>
<p>Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method <code class="docutils literal notranslate"><span class="pre">save_pretrained(save_directory)</span></code> if you were using any other serialization method before.</p>
<p>Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s load a model and tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1">### Do some stuff to our model and tokenizer</span>
<span class="c1"># Ex: add new tokens to the vocabulary and embeddings of our model</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">([</span><span class="s1">&#39;[SPECIAL_TOKEN_1]&#39;</span><span class="p">,</span> <span class="s1">&#39;[SPECIAL_TOKEN_2]&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
<span class="c1"># Train our model</span>
<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1">### Now let&#39;s save our model and tokenizer to a directory</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;./my_saved_model_directory/&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;./my_saved_model_directory/&#39;</span><span class="p">)</span>

<span class="c1">### Reload the model and the tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./my_saved_model_directory/&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./my_saved_model_directory/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules">
<h2>Optimizers: BertAdam &amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules<a class="headerlink" href="#optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules" title="Permalink to this headline">¶</a></h2>
<p>The two optimizers previously included, <code class="docutils literal notranslate"><span class="pre">BertAdam</span></code> and <code class="docutils literal notranslate"><span class="pre">OpenAIAdam</span></code>, have been replaced by a single <code class="docutils literal notranslate"><span class="pre">AdamW</span></code> optimizer which has a few differences:</p>
<ul class="simple">
<li><p>it only implements weights decay correction,</p></li>
<li><p>schedules are now externals (see below),</p></li>
<li><p>gradient clipping is now also external (see below).</p></li>
</ul>
<p>The new optimizer <code class="docutils literal notranslate"><span class="pre">AdamW</span></code> matches PyTorch <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.</p>
<p>The schedules are now standard <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">PyTorch learning rate schedulers</a> and not part of the optimizer anymore.</p>
<p>Here is a conversion examples from <code class="docutils literal notranslate"><span class="pre">BertAdam</span></code> with a linear warmup and decay schedule to <code class="docutils literal notranslate"><span class="pre">AdamW</span></code> and the same schedule:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters:</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">num_training_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">warmup_proportion</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">)</span>  <span class="c1"># 0.1</span>

<span class="c1">### Previously BertAdam optimizer was instantiated like this:</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BertAdam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">schedule</span><span class="o">=</span><span class="s1">&#39;warmup_linear&#39;</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="n">warmup_proportion</span><span class="p">,</span> <span class="n">t_total</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">)</span>
<span class="c1">### and used like this:</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1">### In Transformers, optimizer and schedules are splitted and instantiated like this:</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">correct_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># To reproduce BertAdam specific behavior set correct_bias=False</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">num_warmup_steps</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">)</span>  <span class="c1"># PyTorch scheduler</span>
<span class="c1">### and used like this:</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_grad_norm</span><span class="p">)</span>  <span class="c1"># Gradient clipping is not in AdamW anymore (so you can use amp without issue)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="citation">
<h1>Citation<a class="headerlink" href="#citation" title="Permalink to this headline">¶</a></h1>
<p>We now have a paper you can cite for the 🤗 Transformers library:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@article</span><span class="p">{</span><span class="n">Wolf2019HuggingFacesTS</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">HuggingFace</span><span class="s1">&#39;s Transformers: State-of-the-art Natural Language Processing},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Thomas</span> <span class="n">Wolf</span> <span class="ow">and</span> <span class="n">Lysandre</span> <span class="n">Debut</span> <span class="ow">and</span> <span class="n">Victor</span> <span class="n">Sanh</span> <span class="ow">and</span> <span class="n">Julien</span> <span class="n">Chaumond</span> <span class="ow">and</span> <span class="n">Clement</span> <span class="n">Delangue</span> <span class="ow">and</span> <span class="n">Anthony</span> <span class="n">Moi</span> <span class="ow">and</span> <span class="n">Pierric</span> <span class="n">Cistac</span> <span class="ow">and</span> <span class="n">Tim</span> <span class="n">Rault</span> <span class="ow">and</span> <span class="sa">R</span><span class="s1">&#39;emi Louf and Morgan Funtowicz and Jamie Brew},</span>
  <span class="n">journal</span><span class="o">=</span><span class="p">{</span><span class="n">ArXiv</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2019</span><span class="p">},</span>
  <span class="n">volume</span><span class="o">=</span><span class="p">{</span><span class="nb">abs</span><span class="o">/</span><span class="mf">1910.03771</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
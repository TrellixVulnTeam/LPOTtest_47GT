

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Step-by-Step &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/index.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Step-by-Step</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/examples/pytorch/language_translation/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="step-by-step">
<h1>Step-by-Step<a class="headerlink" href="#step-by-step" title="Permalink to this headline">¶</a></h1>
<p>This document is used to list steps of reproducing PyTorch BERT tuning zoo result.</p>
<blockquote>
<div><p><strong>Note</strong></p>
<ol class="simple">
<li><p>PyTorch quantization implementation in imperative path has limitation on automatically execution.
It requires to manually add QuantStub and DequantStub for quantizable ops, it also requires to manually do fusion operation.
Intel® Low Precision Optimization Tool has no capability to solve this framework limitation. Intel® Low Precision Optimization Tool supposes user have done these two steps before invoking Intel® Low Precision Optimization Tool interface.
For details, please refer to https://pytorch.org/docs/stable/quantization.html</p></li>
<li><p>The latest version of pytorch enabled INT8 layer_norm op, but the accuracy was regression. So you should tune BERT model on commit 24aac321718d58791c4e6b7cfa50788a124dae23.</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="prerequisite">
<h1>Prerequisite<a class="headerlink" href="#prerequisite" title="Permalink to this headline">¶</a></h1>
<div class="section" id="installation">
<h2>1. Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="python-first">
<h3>Python First<a class="headerlink" href="#python-first" title="Permalink to this headline">¶</a></h3>
<p>Recommend python 3.6 or higher version.</p>
</div>
<div class="section" id="install-dependency">
<h3>Install dependency<a class="headerlink" href="#install-dependency" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip install -r requirements.txt
</pre></div>
</div>
</div>
<div class="section" id="install-pytorch">
<h3>Install PyTorch<a class="headerlink" href="#install-pytorch" title="Permalink to this headline">¶</a></h3>
<p>You will need a C++14 compiler. Also, we highly recommend installing an Anaconda environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Dependencies</span>
conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi
<span class="c1"># Install pytorch from source</span>
git clone https://github.com/pytorch/pytorch.git
<span class="nb">cd</span> pytorch
git reset --hard 24aac321718d58791c4e6b7cfa50788a124dae23
git submodule sync
git submodule update --init --recursive
<span class="nb">export</span> <span class="nv">CMAKE_PREFIX_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">CONDA_PREFIX</span><span class="k">:-</span><span class="s2">&quot;</span><span class="k">$(</span>dirname <span class="k">$(</span>which conda<span class="k">))</span><span class="s2">/../&quot;</span><span class="si">}</span>
python setup.py install
</pre></div>
</div>
</div>
<div class="section" id="install-bert-model">
<h3>Install BERT model<a class="headerlink" href="#install-bert-model" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/pytorch/language_translation
python setup.py install
</pre></div>
</div>
<blockquote>
<div><p><strong>Note</strong></p>
<p>Please don’t install public transformers package.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="prepare-dataset">
<h2>2. Prepare Dataset<a class="headerlink" href="#prepare-dataset" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Before running any of these GLUE tasks you should download the <a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running
<a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a> and unpack it to some directory <code class="docutils literal notranslate"><span class="pre">$GLUE_DIR</span></code>.</p></li>
<li><p>For SQuAD task, you should download SQuAD dataset from <a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD dataset link</a></p></li>
<li><p>For language model, you should get a file that contains text on which the language model will be fine-tuned. A good example of such text is the <a class="reference external" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-2 dataset</a>.</p></li>
</ul>
</div>
<div class="section" id="prepare-pretrained-model">
<h2>3. Prepare pretrained model<a class="headerlink" href="#prepare-pretrained-model" title="Permalink to this headline">¶</a></h2>
<p>Before use Intel® Low Precision Optimization Tool, you should fine tune the model to get pretrained model, You should also install the additional packages required by the examples:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/pytorch/language_translation
pip install -r examples/requirements.txt
pip install <span class="nv">torchvision</span><span class="o">==</span><span class="m">0</span>.6.1 --no-deps
</pre></div>
</div>
<div class="section" id="bert">
<h3>BERT<a class="headerlink" href="#bert" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>For BERT base and glue tasks(task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI…)</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
<span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC

<span class="nb">cd</span> examples/pytorch/language_translation
python examples/run_glue_tune.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-uncased <span class="se">\</span>
    --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir /path/to/checkpoint/dir
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.</p>
<p>The dev set results will be present within the text file ‘eval_results.txt’ in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called ‘/tmp/MNLI-MM/’ in addition to ‘/tmp/MNLI/’.</p>
<p>please refer to <a class="reference external" href="README.html#run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification">BERT base scripts and instructions</a>.</p>
<ul class="simple">
<li><p>For BERT large and glue tasks(MRPC, CoLA, RTE, QNLI…)</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
<span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC
<span class="nb">cd</span> examples/pytorch/language_translation
python -m torch.distributed.launch examples/run_glue_tune.py   <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --task_name MRPC <span class="se">\</span>
    --do_train   <span class="se">\</span>
    --do_eval   <span class="se">\</span>
    --do_lower_case   <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/   <span class="se">\</span>
    --max_seq_length <span class="m">128</span>   <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --learning_rate 2e-5   <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0  <span class="se">\</span>
    --output_dir /path/to/checkpoint/dir <span class="se">\</span>
    --overwrite_output_dir   <span class="se">\</span>
    --overwrite_cache <span class="se">\</span>
</pre></div>
</div>
<p>This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &gt; 92.
Training with these hyper-parameters gave us the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8823529411764706
  <span class="nv">acc_and_f1</span> <span class="o">=</span> <span class="m">0</span>.901702786377709
  <span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.3418912578906332
  <span class="nv">f1</span> <span class="o">=</span> <span class="m">0</span>.9210526315789473
  <span class="nv">global_step</span> <span class="o">=</span> <span class="m">174</span>
  <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.07231863956341798
</pre></div>
</div>
<p>please refer to <a class="reference external" href="README.html#fine-tuning-bert-model-on-the-mrpc-classification-task">BERT large scripts and instructions</a></p>
<ul class="simple">
<li><p>For BERT large SQuAD task</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/pytorch/language_translation
python -m torch.distributed.launch examples/run_squad.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
    --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">2</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir /path/to/checkpoint/dir <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">3</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">3</span>   <span class="se">\</span>
</pre></div>
</div>
<p>Training with these hyper-parameters gave us the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python <span class="nv">$SQUAD_DIR</span>/evaluate-v1.1.py <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
<span class="o">{</span><span class="s2">&quot;exact_match&quot;</span>: <span class="m">86</span>.91579943235573, <span class="s2">&quot;f1&quot;</span>: <span class="m">93</span>.1532499015869<span class="o">}</span>
</pre></div>
</div>
<p>please refer to <a class="reference external" href="README.html#run_squadpy-fine-tuning-on-squad-for-question-answering">BERT large SQuAD instructions</a></p>
<ul class="simple">
<li><p>After fine tuning, you can get a checkpoint dir which include pretrained model, tokenizer and training arguments. This checkpoint dir will be used by lpot tuning as below.</p></li>
</ul>
</div>
<div class="section" id="gpt">
<h3>GPT<a class="headerlink" href="#gpt" title="Permalink to this headline">¶</a></h3>
<p>Please download <a class="reference external" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-2 dataset</a>. We will refer to two different files: <code class="docutils literal notranslate"><span class="pre">$TRAIN_FILE</span></code>, which contains text for training, and <code class="docutils literal notranslate"><span class="pre">$TEST_FILE</span></code>, which contains text that will be used for evaluation.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nb">cd</span> examples/pytorch/language_translation
  <span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
  <span class="nb">export</span> <span class="nv">TEST_FILE</span><span class="o">=</span>/path/to/dataset/wiki.test.raw

  python examples/run_lm_tune.py <span class="se">\</span>
      --model_type<span class="o">=</span>openai-gpt <span class="se">\</span>
      --model_name_or_path<span class="o">=</span>openai-gpt <span class="se">\</span>
      --do_train <span class="se">\</span>
      --train_data_file<span class="o">=</span><span class="nv">$TRAIN_FILE</span> <span class="se">\</span>
      --do_eval <span class="se">\</span>
      --eval_data_file<span class="o">=</span><span class="nv">$TEST_FILE</span> <span class="se">\</span>
      --output_dir<span class="o">=</span>/path/to/gpt/checkpoint/dir
</pre></div>
</div>
</div>
<div class="section" id="roberta">
<h3>RoBERTa<a class="headerlink" href="#roberta" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
  <span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC
  
  <span class="nb">cd</span> examples/pytorch/language_translation
  python examples/run_glue_tune.py <span class="se">\</span>
      --model_type roberta <span class="se">\</span>
      --model_name_or_path roberta-base <span class="se">\</span>
      --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --do_train <span class="se">\</span>
      --do_eval <span class="se">\</span>
      --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --max_seq_length <span class="m">128</span> <span class="se">\</span>
      --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>  <span class="se">\</span>
      --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span> <span class="se">\</span>
      --output_dir /path/to/roberta/checkpoint/dir
</pre></div>
</div>
</div>
<div class="section" id="camembert">
<h3>CamemBERT<a class="headerlink" href="#camembert" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
  <span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC
  
  <span class="nb">cd</span> examples/pytorch/language_translation
  python examples/run_glue_tune.py <span class="se">\</span>
      --model_type camembert <span class="se">\</span>
      --model_name_or_path camembert-base <span class="se">\</span>
      --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --do_train <span class="se">\</span>
      --do_eval <span class="se">\</span>
      --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --max_seq_length <span class="m">128</span> <span class="se">\</span>
      --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>  <span class="se">\</span>
      --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span> <span class="se">\</span>
      --output_dir /path/to/camembert/checkpoint/dir
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run">
<h1>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h1>
<div class="section" id="bert-glue-task">
<h2>BERT glue task<a class="headerlink" href="#bert-glue-task" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
<span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC

<span class="nb">cd</span> examples/pytorch/language_translation
python examples/run_glue_tune.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path /path/to/checkpoint/dir <span class="se">\</span>
    --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_eval_batch_size <span class="m">8</span> <span class="se">\</span>
    --no_cuda <span class="se">\</span>
    --tune <span class="se">\</span>
    --output_dir /path/to/checkpoint/dir
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.
Where output_dir is path of checkpoint which be created by fine tuning.</p>
</div>
<div class="section" id="bert-squad">
<h2>BERT SQuAD<a class="headerlink" href="#bert-squad" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/pytorch/language_translation

python examples/run_squad_tune.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path /path/to/checkpoint/dir <span class="se">\</span>
    --task_name <span class="s2">&quot;SQuAD&quot;</span> <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --data_dir /path/to/SQuAD/dataset <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --per_gpu_eval_batch_size <span class="m">16</span> <span class="se">\</span>
    --no_cuda <span class="se">\</span>
    --tune <span class="se">\</span>
    --output_dir /path/to/checkpoint/dir
</pre></div>
</div>
<p>Where output_dir is path of checkpoint which be created by fine tuning.</p>
</div>
<div class="section" id="gpt-wikitext">
<h2>GPT WikiText<a class="headerlink" href="#gpt-wikitext" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
<span class="nb">export</span> <span class="nv">TEST_FILE</span><span class="o">=</span>/path/to/dataset/wiki.test.raw

<span class="nb">cd</span> examples/pytorch/language_translation
python examples/run_lm_tune.py <span class="se">\</span>
    --model_type openai-gpt <span class="se">\</span>
    --model_name_or_path /path/to/gpt/checkpoint/dir <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --train_data_file<span class="o">=</span><span class="nv">$TRAIN_FILE</span> <span class="se">\</span>
    --eval_data_file<span class="o">=</span><span class="nv">$TEST_FILE</span> <span class="se">\</span>
    --no_cuda <span class="se">\</span>
    --tune <span class="se">\</span>
    --output_dir /path/to/gpt/checkpoint/dir
</pre></div>
</div>
<p>Where output_dir is path of checkpoint which be created by fine tuning.</p>
<div class="section" id="roberta-glue-task">
<h3>RoBERTa glue task<a class="headerlink" href="#roberta-glue-task" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
  <span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC
  
  <span class="nb">cd</span> examples/pytorch/language_translation
  python examples/run_glue_tune.py <span class="se">\</span>
      --model_type roberta <span class="se">\</span>
      --model_name_or_path /path/to/roberta/checkpoint/dir <span class="se">\</span>
      --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --do_eval <span class="se">\</span>
      --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --max_seq_length <span class="m">128</span> <span class="se">\</span>
      --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>  <span class="se">\</span>
      --tune <span class="se">\</span>
      --output_dir /path/to/roberta/checkpoint/dir
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.
Where output_dir is path of checkpoint which be created by fine tuning.</p>
</div>
<div class="section" id="camembert-glue-task">
<h3>CamemBERT glue task<a class="headerlink" href="#camembert-glue-task" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
  <span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC
  
  <span class="nb">cd</span> examples/pytorch/language_translation
  python examples/run_glue_tune.py <span class="se">\</span>
      --model_type camembert <span class="se">\</span>
      --model_name_or_path /path/to/camembert/checkpoint/dir <span class="se">\</span>
      --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --do_eval <span class="se">\</span>
      --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
      --max_seq_length <span class="m">128</span> <span class="se">\</span>
      --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>  <span class="se">\</span>
      --tune <span class="se">\</span>
      --output_dir /path/to/camembert/checkpoint/dir
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.
Where output_dir is path of checkpoint which be created by fine tuning.</p>
</div>
</div>
</div>
<div class="section" id="examples-of-enabling-intel-low-precision-optimization-tool">
<h1>Examples of enabling Intel® Low Precision Optimization Tool<a class="headerlink" href="#examples-of-enabling-intel-low-precision-optimization-tool" title="Permalink to this headline">¶</a></h1>
<p>This is a tutorial of how to enable BERT model with Intel® Low Precision Optimization Tool.</p>
</div>
<div class="section" id="user-code-analysis">
<h1>User Code Analysis<a class="headerlink" href="#user-code-analysis" title="Permalink to this headline">¶</a></h1>
<p>Intel® Low Precision Optimization Tool supports two usages:</p>
<ol class="simple">
<li><p>User specifies fp32 ‘model’, calibration dataset ‘q_dataloader’, evaluation dataset “eval_dataloader” and metrics in tuning.metrics field of model-specific yaml config file.</p></li>
<li><p>User specifies fp32 ‘model’, calibration dataset ‘q_dataloader’ and a custom “eval_func” which encapsulates the evaluation dataset and metrics by itself.</p></li>
</ol>
<p>As BERT’s matricses are ‘f1’, ‘acc_and_f1’, mcc’, ‘spearmanr’, ‘acc’, so customer should provide evaluation function ‘eval_func’, it’s suitable for the second use case.</p>
<div class="section" id="write-yaml-config-file">
<h2>Write Yaml config file<a class="headerlink" href="#write-yaml-config-file" title="Permalink to this headline">¶</a></h2>
<p>In examples directory, there is conf.yaml. We could remove most of items and only keep mandotory item for tuning.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">bert</span>
  <span class="nt">framework</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">pytorch</span>

<span class="nt">device</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cpu</span>

<span class="nt">tuning</span><span class="p">:</span>
    <span class="nt">accuracy_criterion</span><span class="p">:</span>
      <span class="nt">relative</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.01</span>
    <span class="nt">exit_policy</span><span class="p">:</span>
      <span class="nt">timeout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
      <span class="nt">max_trials</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">300</span>
    <span class="nt">random_seed</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">9527</span>
</pre></div>
</div>
<p>Here we set accuracy target as tolerating 0.01 relative accuracy loss of baseline. The default tuning strategy is basic strategy. The timeout 0 means early stop as well as a tuning config meet accuracy target.</p>
<blockquote>
<div><p><strong>Note</strong> : lpot does NOT support “mse” tuning strategy for pytorch framework</p>
</div></blockquote>
</div>
<div class="section" id="prepare">
<h2>prepare<a class="headerlink" href="#prepare" title="Permalink to this headline">¶</a></h2>
<p>PyTorch quantization requires two manual steps:</p>
<ol class="simple">
<li><p>Add QuantStub and DeQuantStub for all quantizable ops.</p></li>
<li><p>Fuse possible patterns, such as Conv + Relu and Conv + BN + Relu. In bert model, there is no fuse pattern.</p></li>
</ol>
<p>It’s intrinsic limitation of PyTorch quantizaiton imperative path. No way to develop a code to automatically do that.
The related code changes please refer to examples/pytorch/bert/transformers/modeling_bert.py.</p>
</div>
<div class="section" id="code-update">
<h2>code update<a class="headerlink" href="#code-update" title="Permalink to this headline">¶</a></h2>
<p>After prepare step is done, we just need update run_squad_tune.py and run_glue_tune.py like below</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">tune</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">eval_func_for_lpot</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">result</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  </span><span class="si">%s</span><span class="s2"> = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>
        <span class="n">bert_task_acc_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;best_f1&#39;</span><span class="p">,</span> <span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="s1">&#39;mcc&#39;</span><span class="p">,</span> <span class="s1">&#39;spearmanr&#39;</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">bert_task_acc_keys</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Finally Eval {}:{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>
                <span class="n">acc</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="n">acc</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">evaluate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">output_examples</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">per_gpu_eval_batch_size</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">lpot.experimental</span> <span class="kn">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
    <span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="s2">&quot;./conf.yaml&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">eval_task</span> <span class="o">!=</span> <span class="s2">&quot;squad&quot;</span><span class="p">:</span>
        <span class="n">eval_task</span> <span class="o">=</span> <span class="s1">&#39;classifier&#39;</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">dataset</span><span class="p">(</span><span class="s1">&#39;bert&#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
                                     <span class="n">task</span><span class="o">=</span><span class="n">eval_task</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">model_type</span><span class="p">)</span>
    <span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">eval_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">)</span>
    <span class="n">quantizer</span><span class="o">.</span><span class="n">eval_func</span> <span class="o">=</span> <span class="n">eval_func_for_lpot</span>
    <span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="p">()</span>
    <span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;PATH to saved model&quot;</span><span class="p">)</span>
    <span class="nb">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="original-bert-readme">
<h1>Original BERT README<a class="headerlink" href="#original-bert-readme" title="Permalink to this headline">¶</a></h1>
<p>Please refer <a class="reference internal" href="BERT_README.html"><span class="doc">BERT README</span></a></p>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>